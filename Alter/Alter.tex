\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{algpseudocode}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{Alter Zusammenfassung}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\section{Analyse von Algorithmen}
	\subsection{asymptotische Notation}
	\textbf{Definition} ($\mathcal{O}$-Notation)
	\[\mathcal{O}(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: g(n) \leq c\cdot f(n)\}\]
	\textbf{Definition} ($\Omega$-Notation)
	\[\Omega(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: f(n) \leq c\cdot g(n)\}\]
	\textbf{Definition} ($\Theta$-Notation) \[\Theta(f(n)) = \mathcal{O}(f(n)) \cap \Omega(f(n))\]

	\subsection{worst-case und average-case}
	\textbf{Definition} (worst-case)\\
	Die worst-case Zeit ist die für eine fixe Länge $n$ des Inputs schlechteste Laufzeit eines Algorithmus $A$. \[wc-time_A(n) := \max_{x: \left|x\right| = n} time_A(x)\]
	\textbf{Definition} (average-case)\\
	Bei der average-case Analyse wird angenommen, dass die Eingaben der Länge $n$ gleichverteilt sind. Damit ist die Laufzeit eine Zufallsvariable $T_{A,n}$. \[av-time_{A}(n) := \mathbb{E}[T_{A,n}]\] Mit der Definition des Erwartungswertes der Gleichverteilung also \[av-time_A(n) = \frac{1}{\left|\{x: \left|x\right| = n\}\right|} \cdot \sum_{x: \left|x\right| = n} time_A(n)\]
	\subsection{Abschätzungen}
	Die harmonische Reihe der ersten $n$ Glieder lässt sich abschätzen durch \[H_n = \sum_{i=1}^{n} \frac{1}{i} \leq \ln(n)+1\]
	\textbf{Definition} (Stirling Formel)
	\[n! \sim \sqrt{2\pi n} (\frac{n}{e})^n\]
	wobei $f(n) \sim g(n) \Leftrightarrow \lim_{n\to\infty} \frac{f(n)}{g(n)} = 1$. Daraus folgt auch \[\log(n!) \sim n\log(n) - n\log(e) + \log(\sqrt{2\pi n})\]
	\subsection{Rekursionsgleichungen}
	Zum Beispiel kann die Fibonacci Folge durch $F(n) = \Omega(2^{n/2})$ von unten abgeschätzt werden, da $F(n) \geq 2F(n-1)$.\\
	\textbf{Behauptung}: $F(n) \geq ac^n$ für $a,c >0$.\\ Beweis durch Induktion liefert $c \leq \phi = \frac{\sqrt{5}+1}{2}$. Insgesamt folgt $F(n) = \Theta((\frac{\sqrt{5}+1}{2})^n)$.\\
	\textbf{Beispiel} (Divide and Conquer)\\
	Das zu lösende Problem wird in zwei Teile mit geringerer Größe geteilt. Die Lösung für ein Problem setzt sich aus der Lösung für alle Teilprobleme zusammen. Die Laufzeit ist durch $\mathcal{O}(n\log(n))$ beschränkt. Beispiele für Divide and Conquer Verfahren sind Quicksort, Mergesort oder Binäre Suche.\\
	Mergesort: eine Eingabefolge $a_n$ wird in zwei gleich große Teile gespalten und die beiden Hälften rekursiv sortiert. Im Basisfall werden die beiden Hälften kombiniert und sortiert (merging).\newpage
	\textbf{Algorithmus}
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\State\texttt{INPUT: $x$
			\If{$\left|x\right|\leq 1$}
			\State Berechne die Lösung $L$ für $x$ direkt
			\Else $x = x_1 \cup x_2$
			\State $L_1$ = DivideAndConquer($x_1$)
			\State $L_2$ = DivideAndConquer($x_2$)
			\State Berechne $L$ aus $L_1$ und $L_2$
			\EndIf
			\State RETURN $L$}
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	Für den Aufwand $f$ folgt $f(n) = 2\cdot f(\frac{n}{2}) + c\cdot n + d$.	\\
	\textbf{Behauptung}: $f(n) = \alpha + \beta n +\delta n \log(n)$\\
	Der Beweis folgt durch elementares Nachrechnen. Es folgt durch Koeffizientenvergleich $\alpha = -d$ und $\delta = c$. Für den Mergesort erhält man zum Beispiel $f(n) = n\log(n) - n +1$.\\
	Die Berechnung kann auch durch Iterationsmethode berechnet werden. Dabei wird die Rekursionsgleichung immer wieder in sich selbst eingesetzt. Diese Einsetzung kann $log(n)$ oft geschehen, dann ist der Basisfall 1 erreicht. Mittels der $k$-ten Partialsumme der geometrischen Reihe, kann die Laufzeit bestimmt werden.\\
	\textbf{Beispiel} (Andere Rekursionen)\\
	Zum Beispiel für Probleme der Form \[f(n) = 2f(\sqrt{n}) + \log(n)\] hilft Iteration nicht weiter. Aber man kann sich durch Variablensubstitution behelfen. Mit $n = 2^m$ erhält man \[f(2^m) = 2f(2^{m/2}) + m\] Mit $f(2^m) = g(m)$ also \[g(m) = 2g(\frac{m}{2}) + m\]
	\textbf{Vereinfachung}:\begin{itemize}
		\item ganzzahlige Rekursionen $f(n) = f(\lfloor n\rfloor) + f(\lceil n \rceil)$ können ignoriert werden und von der Form $f(n) = 2f(\frac{n}{2})$ geschrieben werden.
		\item Ignorieren der Anfangswertbedingung
		\item Nur $\Omega$ und $\mathcal{O}$ Notation bestimmen
		\item Abschätzungen, die nur für große $n$ gelten
		\item \underline{Vorsicht:} asymptotische Notation in der Rekursiongleichung kann zu Problemen führen. 
	\end{itemize}
	\subsection{Master Theorem}
	\textbf{Satz}\\
	Gegeben eine Rekursiongleichung der Form \[T(n) = \sum_{i=1}^m T(\alpha_i\cdot n) + \Theta(n^k)\] wobei $0 \leq \alpha_i \leq 1$, $m\geq 1$ und $k\geq 0$. Dann kann man $T(n)$ abschätzen durch \[T(n) = \begin{cases}
		\Theta(n^k), & \text{falls } \sum_{i=1}^m \alpha_i^k < 1\\
		\Theta(n^k \log n), & \text{falls } \sum_{i=1}^m \alpha_i^k = 1\\
		\Theta(n^c), & \text{sonst}
	\end{cases}\]
	$c$ im letzten Fall ergibt sich dabei durch Lösen der folgenden Gleichung: \[\sum_{i=1}^m \alpha_i^c = 1\] Wenn alle $\alpha_i$ gleich sind, ergibt sich der Spezialfall $c = \frac{-\ln m}{\ln \alpha}$\\
	\textbf{Selektionsalgorithmen}\\
	Ein Selektionsalgorithmus soll für eine Folge $a_n$ und ein $i$ feststellen, welches das $i$-kleinste Element von $a_n$ ist. Zum Beispiel für $i=n$ das Maximum, $i=1$ das Minimum und $i=\frac{n}{2}$ der Median. Der naive Ansatz, die Liste zuerst zu sortieren, kann aber laufzeittechnisch verbessert werden. Als Komplexitätsmaß wird die Vergleichskomplexität $V(n)$ herangezogen. Im worst-case muss jeder Sortieralgorithmus so viele Vergleiche machen, wie man benötigt, um $n!$ durch halbieren zu einer Zahl $\leq 1$ zu machen. Mit der Stirling Formel ergibt sich, dass jeder vergleichsbasierte Sortieralgorithmus eine worst-case Laufzeit von $\geq n\log n - \Theta(n)$ hat. Die Darstellung eines solches Verfahren kann mit einem Entscheidungsbaum geschehen.\\
	\underline{\textbf{Heap Sort}}\\
	Ein Heap ist ein Binärbaum mit folgenden beiden Eigenschaftem \begin{itemize}
		\item der Baum ist soweit es geht vollständig gefüllt, die unterste Stufe ist von links gefüllt.
		\item Die Knoten haben Zahlen. Die Kindknoten eines beliebigen Knotens haben Werte kleiner oder gleich des Wertes des Elternknotens.
	\end{itemize}
	Der HeapSort läuft in zwei Phasen: \begin{enumerate}
		\item die Zahlenfolge wird zu einem Heap gebaut
		\item der Heap wird abgebaut, indem die Wurzel entfernt wird und ein anderer Knoten an ihre Stelle gesetzt wird, sodass die Heap Eigenschaft weiter erfüllt ist.
	\end{enumerate}
	HeapSort ist ein Inplace-Sortierverfahren. Die worst-case-Laufzeit beträgt $\mathcal{O}(n\log n)$.\\
	\underline{\textbf{Quicksort}}\\
	Wir betrachten Quicksort, ein stabiles divide-and-conquer Verfahren.
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\State\texttt{PROCEDURE} Quicksort($a_1,...,a_n$)
		\If{$n=0$}
			\State RETURN
		\EndIf
		\State $x\gets a_1$
		\State $l \gets$ leere Folge
		\State $r \gets$ leere Folge
		\For{$i=2$ TO $n$}
			{\If{$a_i \leq x$}
				\State{append $a_i$ to $l$} 
			\Else
				\State {append $a_i$ to r}
			\EndIf}
		\EndFor
		\State RETURN (QuickSort(l),$x$,QuickSort(r))
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\underline{Eigenschaften} \begin{itemize}
		\item im schlechtesten Fall wird jedes Mal die größte (oder kleinste) Zahl als Pivotelement gewählt
		\item Dann hat die $i$-te Teilfolge Läge $n-i$
		\item im schlimmsten Fall also eine Laufzeit von $\Theta(n^2)$
		\item im best-case ist das Pivotelement der Median
		\item die $i$-te Teilfolge hat dann Länge $\frac{n}{2^i}$ 
		\item das bringt eine Laufzeit von $n\log n + \Theta(n)$
	\end{itemize}
	\subsection{Average-Case-Analyse von Quicksort}
	Annahme: die Eingabe Liste ist gleichverteilt und paarweise verschieden.\\
	Die Eingabefolge sein $a_n$ und die sortierte Folge $s_n$. Wir führen eine Zufallsvariable $X_{ij}$ ein, wobei $X_{ij} = 1$, falls beim Quicksort $s_i$ mit $s_j$ verglichen werden. Daher ist die mittlere Anzahl an Vergleichen gegeben durch \[V(n) = \sum_{i<j}\mathbb{E}[X_{ij}]\]
	Nun ist $\mathbb{E}[X_{ij}] = p_{ij} + 0\cdot (1-p_{ij}) = p_{ij}$ die Wahrscheinlichkeit, dass $s_i$ und $s_j$ verglichen werden. Man findet, dass $p_{ij} = \frac{2}{j-i+1}$. Insgesamt also \[V(n) = \sum_{i<j} \mathbb{E}[X_{ij}] = \sum_{i<j} \frac{2}{j-i+1}\] Durch elementares Umformen findet man $V(n) \leq 1,3863n \log_2 n$. Die obige Implementierung von Quicksort ist nicht in-place, denn es werden immer neue (Teil-)Folgen generiert.
	\section{Selektionsalgorithmen}
	Gegeben sei eine Folge von Zahlen. Es soll das $i$-kleinste Element gefunden werden für $i \in [n]$. Bei $i=1$ das Minimum, bei $i=n$ das Maximum, bei $i = \frac{n}{2}$ den Median.\\
	\subsection{Spezialfall Extrema (z.B. Maximum)} 
	Die naive Implementierung braucht lineare Zeit und genauer $V(n) = n-1$ Vergleiche. Diese Implementierung ist optimal, denn bei weniger als $n-1$ Vergleichen gibt es ein Element, das nie überprüft wurde.
	\subsection{Maximum und Minimum}
	Die naive Implementierung bestimmt die Extrema separat und benötigt daher $2n-2$ Vergleiche. Dieser Wert lässt sich verbessern:\\ Es wird immer ein Element mit seinem Nachfolger verglichen. Dabei gibt es $\frac{n}{2}$ größere und $\frac{n}{2}$ kleinere Elemente. Für diese Partition braucht man daher $\frac{n}{2}$ Vergleiche. Unter den kleineren kann das Minimum und unter den größeren das Maximum in jeweils $\frac{n}{2}-1$ Vergleichen bestimmt werden. Insgesamt folgt also $V(n) = \frac{3}{2}n-2$. Man kann zeigen, dass dieses Vorgehen optimal ist.
	\subsection{Selektion des $k$-kleinsten Elementes}
	Die Vorgehenweise ist angelegt an Quicksort. Folgende Prozedur löst das Problem:\\
	Select($a,l,r,i$) bestimmt das $i$-kleinste Element in der Teilfolge $a[l,r]$.\\
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\If{$i+l-1\notin \{l,...,r\}$} 
		\State return fail
		\EndIf
		\State Bestimmte Pivotelement $x$ aus $a[l,...,r]$
		\State Sei $m$ der Rang von $x$ in $a[l,...,r]$
		\State sortiere $a$ so um, dass $a[l+m-1]=x \land \begin{cases}
			a[j] < a[m+l-1] & \text{ für } l\leq j<m+l-1\\
			a[j] > a[m+l-1] & \text{ für } r \geq j >m+l-1 
		\end{cases}$
	\If{$i=m+l-1$}
	\State return $x$
	\EndIf
	\If{$i<m+l-1$}
	\State return Select($a,l,l+m-2,i$)
	\EndIf
	\If{$i>m+l-1$}
	\State return Select($a,l+m,r,i-m$)
	\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\textbf{Worst-Case-Komplexität}\\
	Analog zum Quicksort tritt der schlechteste Fall ein, wenn das Pivotelement am Rand liegt. Dann ist die Komplexität $\mathcal{O}(n^2)$.\\
	\textbf{Random Select}\\
	Angenommen das Pivotelement wird zufällig gemäß einer Gleichverteilung gewählt. Ist $x$ das $j$-kleinste Element so hat das linke Teilarray die Größe $j-1$ und das rechte $n-j$. Im schlimmsten Fall wird Select rekursiv mit dem größeren Teilarray aufgerufen. die Aufteilung benötigt $n-1$ Vergleiche. D.h. die mittlere Anzahl von Vergleichen ist rekursiv beschränkt durch $V(n) \leq (n-1) + \frac{2}{n} \sum_{j = n/2}^{n-1} V(j)$. Durch Einsetzen kann man diese Rekursionsungleichung lösen und findet, dass der average case linear ist.\\
	\textbf{Pivotelement geschickt wählen}\\
	Man organisiert die Liste in Fünfergruppen und sortiert diese (jeweils konstant mit genau 7 Vergleichen). In jeder Fünfergruppe wird der Median bestimmt und anschließend der Median dieser Mediane. Dieser wird als Pivotelement gesetzt. Damit hat sich die Anzahl der zu vergleichenden Elemente auf $\frac{4}{10}n$. Es ergibt sich die Rekursionsgleichung \[V(n) = \frac{7n}{5} + V(\frac{n}{5} + \frac{4n}{10}) + V(\frac{7n}{10})\] Mittels Mastertheorem lässt sich diese Gleichung lösen und man findet, dass die Prozedur mit dieser Pivotwahl linearen Aufwand hat.
	\section{Hashing}
	Hashing ist ein System zur dynamischen Verwaltung von Daten, die über einen eindeutigen Schlüssel angesprochen werden. Das Speicherverfahren bietet einige Vorteile:\begin{itemize}
		\item Die Daten können in Bäumen verwaltet werden, wodurch die Dictionary Operations Aufwand von $\mathcal{O}(\log n)$ erhalten. Beim Hashing beschränkt sich dieser Aufwand auf $\mathcal{O}(1)$.
		\item Hilfreich, wenn es viele potentielle Schlüssel $U$ gibt und
		\item die Menge $S$ der Schlüssel aber klein im Vergleich zu $U$ ist.
		\item Außerdem, wenn $S$ im Vorhinein nicht bekannt ist.
	\end{itemize}	
	\textbf{Definition}\\
	Eine Hashtabelle ist ein Array von Zeigern auf die eigentlichen Datensätze. \\
	\textbf{Definition}\\
	Eine Hashfunktion $h$ bildet die Schlüssel auf natürliche Zahlen ab.\\
	
	Wenn die Hashfunktion injektiv ist, können die Dictionary Operations Suchen, Einfügen, Entfernen direkt implementiert werden. Gilt für $s,s' \in S$, dass $h(s) = h(s')$, so spricht man von einer Kollision.
	\subsection{Hashfunktionen}
	\subsubsection{Divisions-/Kongruenzmethode}
	$h(s) = s \mod m$, im besten Fall sollte $m$ prim sein.
	\subsubsection{Multiplikationsmethode}
	$h(s) = \lfloor m \cdot ((s\cdot \alpha) \mod 1)\rfloor$ für eine reelle Zahl $\alpha \in [0,1]$. $\mod 1$ soll den ganzzahligen Anteil streichen und $m = 2^p$ für eine natürliche Zahl $p$.\\
	
	Kollisionen lassen sich meist nicht verhindern. Das kann als Analogie zum Geburtstagsparadoxon gesehen werden. Für eine Hashtabelle der Größe $m$ und $n$ Schlüssel können wir die Anzahl der Kollisionen mit folgender Zufallsvariablen abschätzen: \[X_{i,j} = \begin{cases}
		1, & h(s_i) = h(s_j)\\
		0, & \text{ sonst}
	\end{cases}\]
	Für $i\neq j$ gilt dann $\mathbb{E}[X_{i,j}] = \frac{1}{m}$. Sei $X = \sum_{i<j} X_{i,j}$, dann gilt $\mathbb{E}[X] = \binom{n}{2} \frac{1}{m}$. 
	\subsection{Belegungsfaktor}
	Der Quotient $\beta = \frac{n}{m}$ heißt der Belegungsfaktor einer Hashtabelle der Größe $m$, wobei $n$ die Anzahl der gespeicherten Elemente ist. Der Suchaufwand lässt sich als Funktion in $\beta$ abschätzen.
	\subsection{Hashing mit Verkettung}
	Jedes Element der der Hashtabelle ist Ausgangspunkt einer Überlaufliste. In dieser Liste werden wiederum linked lists gespeichert, deren Schlüssel den selben Hashwert erzeugen. In diesem Fall sind die Dictionary Operations Initialisierung, Suchen, Einfügen und Löschen leicht realisierbar. Für die Komplexitätsanalyse der Suche unterscheiden wir zwei Komplexitätsfunktionen: \begin{enumerate}
		\item $A(n,m)$, die mittlere Anzahl der Sondierschritte, vorausgesetzt der Schlüssel ist in der Tabelle eingetragen.
		\item $A'(n,m)$, vorausgesetzt der Schlüssel ist nicht in der Tabelle eingetragen. 
	\end{enumerate}
	Man kann zeigen, dass der Aufwand bei erfolgloser Suche $A' = 1+\beta$ beträgt und bei erfolgreicher Suche durch $A \leq 1+\frac{\beta}{2}$ abgeschätzt werden kann.
	\subsection{Open Hashing}
	Hierbei können maximal $n=m$ Elemente verwaltet werden, d.h. $\beta \leq 1$. Das Löschen fällt in dieser Form schwerer, da die Sondierreihenfolge durch eine gelöschte Stelle unterbrochen werden kann. Eine Möglichkeit besteht darin, zwischen freien und gelöschten Stellen zu unterscheiden. Der Suchaufwand beim linearen Sondieren beträgt $A \approx \frac{1}{2} \left(1+ \frac{1}{1-\beta} \right)$ bzw. $A' = \frac{1}{2} \left(1+\frac{1}{(1-\beta)^2}\right)$. 
	\subsection{Double Hashing}
	Zur Vermeidung von Clusterbildung legt eine zweite Hashfunktion $h'$ die Schrittweite für die Sondierung fest. Zunächst wird $h(s)$ sondiert und dann iterativ die Adressen $(h(s) + i\cdot h'(s)) \mod m$ für $i \in \mathbb{N}$ nach einem freien Platz abgesucht. Lineares Sondieren ist hier also der Spezialfall $h' \equiv 1$. $h$ und $h'$ sollten sich möglichst stark voneinander unterscheiden. Eigenschaften beim Double Hashing \begin{itemize}
		\item Es ist wichtig, dass $m$ eine Primzahl ist
		\item Wenn $m$ und $h'(s)$ nicht teilerfremd sind, so wird beim Sondieren nicht die gesamte Tabelle durchlaufen
		\item $A' = \frac{1}{1-\beta}$
		\item $A = -\frac{1}{\beta} \cdot \ln(1-\beta)$
	\end{itemize}
	\subsection{Bloomfilter}
	Das ist eine Datenstruktur, mit der effizient festgestellt werden kann, ob ein Element bereits gespeichert wurde oder nicht. Er besteht aus einem Bitarray der Größe $m$ das mit Nullen initialisiert wird und $k$ unterschiedlichen Hashfunktionen mit $h_i: U \to \{0,...,m-1\}$.\\
	Soll nun $x$ gespeichert werden, so werden zunächst die $k$ Hashwerte $h_i(x)$ bestimmt. Jede dieser Positionen im Bitarray $A$ wird auf 1 gesetzt.\\
	Nun soll überprüft werden, ob ein Wert $w$ gespeichert ist. \begin{enumerate}
		\item Zuerst werden wieder die $k$ Hashwerte bestimmt.
		\item Enthält eine der Positionen eine 0, so ist $w$ nicht gespeichert worden.
		\item Enthalten alle eine 1, so ist es wahrscheinlich, dass $w$ gespeichert wurde, aber nicht sicher (falsch positive möglich). 
	\end{enumerate} 
	\section{Funktionale Programmierung}
	\textbf{Beispiel}\\
	Der Binomialkoeffizient lässt sich rekursiv wie folgt berechnen: \[\binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}\]
	Setzt man $b(n,k) = \binom{n}{k}$ erhält man folgende Rekursionsvorschrift \[b(n,k) = \begin{cases}
		1,& \text{falls } k=0 \lor n=k\\
		b(n-1, k-1) + b(n-1, k),& \text{sonst}
	\end{cases}\] 
	Mittels dieses Verfahrens erhält man einen Aufwand von $\Theta(\frac{2^n}{\sqrt{n}})$. Der Aufwand kann reduziert werden, wenn die schon berechneten Werte in einer Tabelle $t[n,k]$ gespeichert werden. Wir können die Anzahl der Additionen auf $n*k$ reduzieren.\\
	
	Im Gegensatz zu den rekursiven top-down Ansätzen wird beim dynamischen Programmieren die Lösung in bottom-up berechnet. Begonnen wird bei den Randwerten, die späteren Lösungen werden rekursiv aus den kleineren Teillösungen berechnet. Im Beispiel des Binomialkoeffizienten erhält man das Pascal'sche Dreieck. Allgemein ist der Aufwand bei dynamischer Programmierung beschränkt durch (Größe der Tabelle)(Aufwand zur Berechnung eines Eintrages). Eingesetzt wird dieses Prinzip meist zur Lösung von Optimierungsproblemen. Dafür ist es wichtig, dass ein Problem dem Bellmanschen Optimalitätsprinzip folgt: \begin{quote}
		Die optimale Lösung für Probleme setzt der Größe n setzt sich aus optimalen Lösungen für kleinere Teilprobleme zusammen.
	\end{quote}
	Die Entwicklung eines Algorithmus erfolgt in mehreren Schritten \begin{enumerate}
		\item Charakterisiere den Lösungsraum und die Struktur einer optimalen Lösung
		\item Definiere rekursiv, wie sich eine optimale Lösung aus optimalen Teillösungen ermitteln lässt.
		\item Konzipiere eine Algorithmus, der einer Tabelle verwendet.
	\end{enumerate}
	\textbf{Definition}\\
	Die Editierdistanz $d(S_1, S_2)$ ist die minimale Anzahl an Buchstabenoperationen, die nötig sind, um den String $S_1$ in $S_2$ umzuwandeln. Man kann Buchstaben einsetzen, löschen und ersetzen. Mit der naiven Strategie stellt man fest, dass der Aufwand exponentiell wächst, während man mittels dynamischer Programmierung eine Komplexität von $\mathcal{O}(n^3)$ erreicht.\\
	\subsection{Traveling Salesman Problem}
	Das Problem ist \textbf{NP}-vollständig. Der naive Ansatz hat eine Komplexität von $\Omega(n!)$. Wir betrachten im Folgenden das Problem in einem gerichteten Graphen, d.h. die Entfernungsmatrix ist i.A. nicht symmetrisch und die Dreiecksungleichung muss nicht erfüllt sein. Wir betrachten das Problem rekursiv: Wir beginnen bei Stadt $1$, finden eine Rundreise in $\{2,...,k-1\}$ reisen zu $k$ und dann zurück zu $1$. Durch dieses Verfahren wird der Aufwand beschränkt durch $\mathcal{O}(n^22^n)$.
	\subsection{Knapsack Problem}
	Auch dieses Problem ist \textbf{NP}-vollständig. Sei $I$ die Menge der Indizes, sodass die Lösung, die $I$ entspricht optimal sei. Für $i \in I$ ist das Problem mit Größe $G-g_i$ und Indexmenge $I\setminus\{i\}$ optimal gefüllt. Mittels dynamischer Programmierung kann man eine Komplexität von $\mathcal{O}(n2^k)$ erreichen.
	\section{Greedy Algorithmen}
	\textbf{Definition}\\
	Solche Algorithmen sind charakterisiert durch \begin{itemize}
		\item die Menge der möglichen Eingaben
		\item zulässige Lösungen
		\item eine Bewertungsfunktion
	\end{itemize}  
	Eine optimale Teillösung wird durch lokal verfügbare Informationen erweitert. Es wird jeweils die lokal beste Erweiterung gewählt.\\
	
	Eine mögliche Anwendung eines Greedy Algorithmus ist das Bruchteil Rucksack Problem. Sortiert man die Objekte der Größe nach, wählt der Greedy Algorithmus immer das beste noch nicht gewählt Element.\\
	Eine weitere Anwendung von Greedy Algorithmen ist Kruskal für Minimum Spanning Trees.
	\subsection{Union-Find-Datenstruktur}
	\textbf{Definition}
	\begin{itemize}
		\item jeder Menge wird ein kanonisches Element zugeordnet
		\item \texttt{Union}($x,y$) vereinigt die Mengen, die $x$ und $y$ enthalten. Als kanonisches Element kann kann jedes Element der neu entstandenen Menge gewählt werden.
		\item \texttt{Union}($x$) findet das kanonische Element der Menge zu de r$x$ gehört.
		\item \texttt{Make}($x$) erzeugt eine einelementige Menge $\{x\}$ mit kanonischem Element $x$.
	\end{itemize}
	Der Kruskal Algorithmus kann mittels der Union Find Datenstruktur leicht implementiert werden. Sei $f(n)$ die Komplexität der \texttt{Find}-Funktion und $u(n)$ die der \texttt{Union}-Funktion. Dann ist die Komplexität des Kruskal Algorithmus beschränkt durch $\mathcal{O}(m(\log m + f(n) + u(n))$. \begin{itemize}
		\item Die disjunkten Mengen kann man als Baum implementieren
		\item Die Knoten entsprechen den Elementen
		\item Die Wurzel ist das kanonische Element
	\end{itemize}
	Die Operationen kann man sich anhand von Bäumen überlegen:\begin{itemize}
		\item \texttt{Make}($v$) erzeugt einen Baum mit lediglich einer Wurzel
		\item \texttt{Union}($u,v$) vereinigt zwei Bäume indem die Wurzel eines Baums als Kind an einen anderen Baum angehängt wird.
		\item \texttt{Find}($v$) wandert vom Knoten $v$ bis zur Wurzel.
	\end{itemize}
	Für eine effiziente Implementierung werden die Bäume als Arrays gespeichert. Die Komponenten werden in einem Array $A$ und die Größe jeder Komponenten in einem anderen Arrays $S$ gespeichert. \begin{itemize}
		\item Setze $A[i] = i$ und $S[i] = 1$ für $1 \leq i \leq n$
		\item Um \texttt{Find}($i$) zu berechnen, wird $A[i]$ mit $i$ verglichen. Falls $A[i]=i$, so ist das Ergebnis $i$, sonst ist es \texttt{Find}$(A[i])$.
		\item \texttt{Union}($i,j$) wird durch eine Fallunterscheidung der Größen $S[i]$ und $S[j]$ implementiert. Der kleinere Baum wird unter den größeren angehängt.
	\end{itemize}
	Durch diese Implementierung hat ein Baum nach $n$ \texttt{Union}-Operationen genau $n+1$ Knoten und ist nicht tiefer als $\log_2(n+1)$. Die Komplexität der \texttt{Find}-Operation hat nun Komplexität $\mathcal{O}(\log n)$ und \texttt{Union} $\mathcal{O}(1)$. Daher können wir Kruskal durch $\mathcal{O}(m\log m)$ beschränken.\\
	
	Die \texttt{Find}-Operation kann mittels Pfadoptimierung verbessert werden. Die Einträge des Arrays, die bis zur Wurzel durchlaufen wurden, werden mit dem Wert des Wurzelknotens überschrieben. Dadurch entsteht also ein Baum mit der Tiefe 1.
	\subsection{Kürzeste Wege in Graphen}
	Gegeben sei ein gewichteter Graph $G$ der von einem Startknoten $v$ durchquert werden soll. Gesucht sind die kürzesten Wege von $v$ zu allen anderen Knoten.\\
	Dijkstra's Algorithmus ist ein Greedy-Löser. Die Komplexität ist beschränkt durch $\mathcal{O}(n^2)$ in der naiven Implementierung.\\
	
	Mit einem Heap lässt sich folgendermaßen eine bessere Laufzeit erreichen: Aufbau des Heaps wie bei Heapsort in $\mathcal{O}(n)$, Entfernen des Minimums und Aufrufen von heapify in $\mathcal{O}(\log n)$, Update der $l(v)$ Werte in $\mathcal{O}(\log n)$.
	\subsection{Repräsentation von Graphen}
	Wir betrachten zwei Möglichkeiten, um einen Graphen kompakt zu speichern: \begin{itemize}
		\item Adjazenzlisten: an der $k$-ten Stelle der Liste sind Zeiger auf die Nachbarn des Knoten $k$ gespeichert
		\item Adjazenzmatrix: wie gewohnt.
	\end{itemize}
	Die Adjazenzliste erlaubt das Durchlaufen der $k$ Nachbarn eines Knotens in $\mathcal{O}(k)$, was bei einer Adjazenzmatrix nicht möglich ist. Über diese Implementierung kann man Dijkstra mit $\mathcal{O}((n+m)\log n)$ ausführen.
	
	\section{Datenkompression}
	\subsection{Texte}
	Statt Zeichen durch gleich lange Wörter zu speichern, kann man mittels einer Häufigkeitsanalyse eine komprimierte Version speichern. Häufig auftretende Zeichen mit kürzeren und seltene mit längeren Codewörtern.\\
	\textbf{Definition}\\
	Gegeben sind ein Alphabet $\Sigma$ und eine Wahrscheinlichkeitsverteilung $P$ auf $\Sigma$, die jedem $a_i \in \Sigma$ eine Wahrscheinlichkeit $p_i$ zuordnet. \\
	Gesucht ist ein eindeutig decodierbarer Code $c$, d.h. $\hat{c}$ ist injektiv. Die mittlere Codelänge von $c$ ist $\sum_{a_ \in \Sigma} p_i \cdot \left|c(a_)\right|$. Ein Code ist optimal, wenn die mittlere Codelänge minimal ist.\\
	\textbf{Definition} (Präfixcode)\\
	Ein Präfixcode $c$ ist ein Code, bei dem kein Codewort ein Präfix eines anderen Wortes ist. Diese Codes sind eindeutig decodierbar. Anfang und Ende der Codewörter können im komprimierten Code eindeutig identifiziert werden.\\
	\textbf{Definition} (Codebaum)\\
	Das sind Binärbäume, die die Darstellung von Präfixcodes erlauben. In jedem Knoten entspricht die linke Kante einer 0, die rechte einer 1. Jeder Code lässt sich als Codebaum darstellen. Bei Präfixcodes sind die Codewörter ausschließlich an den Blättern zu finden. Zum Decodieren wird ein Code solange im Baum nachvollzogen, bis ein Blatt erreicht wird, dann wird wieder an der Wurzel begonnen.
	\subsection{Huffman-Codes}
	Initialisierung: Jedes Zeichen $a_i$ wird als Baum aufgefasst, dessen Bewertung ist $p_i$. Dann wird folgendes Verfahren wiederholt bis nur ein Baum übrig ist:\begin{enumerate}
		\item zwei Bäume mit kleinster Bewertung werden zu einem Baum zusammengefasst.
		\item Der neue Baum besteht aus einem Knoten als Wurzel und den beidem Teilbäumen als Nachkommen. 
		\item Die Bewertung des neuen Baumes ist die Summe der Bewertung der beiden Teilbäume.
	\end{enumerate} 
	Die Teilbäume werden in einer Priority-Queue verwaltet. Dies kann z.B. ein Heap sein. \begin{itemize}
		\item Initialisieren $\mathcal{O}(n)$
		\item Zusammenfassen bringt zwei Teilbäume, d.h. $n-1$ Wiederholungen
		\item Jedes Zusammenfassen erfordert $\mathcal{O}(\log n)$
	\end{itemize}
	D.h. insgesamt $\mathcal{O}(\log n)$.\\
	\textbf{Theorem}\\
	Der Algorithmus nach Huffman erzeugt einen optimalen Präfixcode. Für natürliche Sprachen erreicht man eine Kompressionsrate von ca. $\frac{2}{3}$.
	\section{Algorithmen auf Graphen}
	\subsection{Repräsentation von Graphen}
	Wir kennen schon Adjazenzlisten und Adjazenzmatrizen. Die Verwendung ist abhängig von der Situation und von Einschränkungen an die Repräsentation. Listen wurden z.B. in Dijkstra's Algorithmus verwendet während man in Algorithmen auf vollständigen Graphen eher zu Matrizen tendieren würden.\\
	
	Listen speichern nur tatsächlich vorhandene Kanten. Allen Nachbarn zu untersuchen ist leicht, aber Vorgänger eines Knotens zu finden ist umständlich. Speicherplatz ist $\mathcal{O}(n+m)$.\\
	Bei Matrizen ist der Test, ob eine Kante existiert in konstanter Zeit möglich, Nachbarschaftsanalysen sind schwierig. Der Speicherplatz beträgt $\mathcal{O}(n^2)$.
	\subsection{Durchsuchen eines Graphen}
	Es bieten sich zwei Strategien, Breiten- und Tiefensuche an.
	\subsection{Breitensuche}
	Wir verwenden Dijkstra mit ungewichteten Kanten: \begin{itemize}
		\item Als Datenstruktur wird eine Queue (FIFO) verwendet.
		\item Diese enthält zunächst den Startknoten $u$.
		\item Es wird jeweils der erste Knoten der Schlange entnommen, die Nachbarn betrachtet und hinten angefügt.
		\item Damit ein Knoten nicht mehrmals eingefügt wird, kann die Distanz $d$ herangezogen werden, wenn diese nicht unendlich ist, wurde ein Knoten bereits betrachtet.
	\end{itemize}
	Da das Verfahren nur Dijkstra ist, ist die Komplexität beschränkt durch $\mathcal{O}((n+m)\log n)$. Da man aber keinen Heap benötigt, reicht sogar $\mathcal{O}(n+m)$.
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Breitensuche($G,u$)}
	\begin{algorithmic}[1]
		\For{$v \in V$}
		\State $d(v) = \infty$
		\EndFor
		\State $d(u) = 0$
		\State $Q = \{u\}$
		\While{$Q \neq \emptyset$}
		\State $v$ = erstes Element aus $Q$
		\State entferne $v$ aus $Q$
		\For{$v' \in N(v)$}
		\If{$d(v') = \infty$}
		\State $d(v') = d(v')+1$
		\State $Q = Q\cup \{v'\}$
		\EndIf
		\EndFor
		\EndWhile
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\subsection{Tiefensuche}
	Der Pfad wird in die Tiefe verlängert. Für einen Knoten $v$ wird die Tiefensuche rekursiv für alle $u \in N(v)$ aufgerufen. Die Komplexität ist auch hier beschränkt durch $\mathcal{O}(n+m)$.
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Tiefensuche($v$)}
	\begin{algorithmic}[1]
		\State $f(u) = v$
		\For{$v \in N(u)$}
		\If{$f(v) =$ weiß}
		Tiefensuche($v$)
		\EndIf
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\subsection{Topologische Sortierung}
	Ein Graph ist topologisch sortiert, wenn eine Nummerierung der Knoten vorliegt, sodass $v_iv_j$ eine Kante ist genau dann, wenn $i<j$.\\
	\textbf{Lemma}\\
	Ein Graph ist topologisch sortierbar genau dann, wenn er azyklisch ist. Die einfachste Methode der topologischen Sortierung ist folgender Algorithmus:
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}[1]
		\For{$v \in V$}
		\State $f(v) =$ weiß
		\EndFor
		\For{$v \in V$}
		\If{$f(v) =$ weiß}
		Tiefensuche($v$)
		\EndIf
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\subsection{Transitive Hülle}
	Die Transitive Hülle eines Graphen ist definiert als die Kantenmenge $E'$, sodass $u,v \in E' \; \Leftrightarrow \exists u-v$-Pfad in $G$.
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}[1]
		\State $E' = \emptyset$
		\For{$u \in V$}
		\State Breitensuche($G, u$)
		\For{$v \in V$}
		\If{$d(v) \neq \infty$}
		\State $E' = E' \cup \{uv\}$
		\EndIf
		\EndFor
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Mittels dynamischem Programmieren lässt sich die transitive Hülle in $\mathcal{O}(n^3)$ finden. Dafür verwenden wir den Warshall Algorithmus:
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Warshall Algorithmus}
	\begin{algorithmic}[1]
		\For{$k = 1$ to $n$}
		\For{$i = 1$ to $n$}
		\If{$A[i,k] = 1$}
		\For{$j = 1$ to $n$}
		\If{$A[k,j] = 1$}
		\State $A[i,j] = 1$
		\EndIf
		\EndFor
		\EndIf
		\EndFor
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	Wobei $A$ hierbei die Adjazenzmatrix von $G$ sei.
	\subsection{Kürzeste Wege}
	Eine Modifikation des Warshall Algorithmus kann die kürzeste Wege zwischen allen Knotenpaaren eines Graphen berechnen. Das unterscheidet sich von Dijkstra, da hier lediglich die Kürzesten Wege von einem Startknoten aus bestimmt werden. Wir definieren nun analog zur Adjazenzmatrix $A$ die Gewichtmatrix $D$: \[D[i,j] = \begin{cases}
		0, & i=j\\
		w(i,j), & ij \in E\\
		\infty, & \text{sonst}
	\end{cases}\]
	Dann können wir die kürzesten Wege mithilfe dem folgenden Algorithmus bestimmen: 
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Algorithmus von Floyd}
	\begin{algorithmic}[1]
		\For{$k = 1$ to $n$}
		\For{$i = 1$ to $n$}
		\For{$j = 1$ to $n$}
		\If{$D[i,k]+D[k,j] < D[i,j]$}
		\State $D[i,j] = D[i,k] + D[k,j]$
		\EndIf
		\EndFor
		\EndFor
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\subsection{Flüsse in Netzwerken}
	Gegeben ist ein gerichteter Graph, eine Quelle $s$ und eine Senke $t$. Das Kantengewicht ist die Kapazität.\\
	Ein \underline{Fluss} ist eine Funktion definiert auf $V\times V$ für die gilt \begin{itemize}
		\item $f(u,v) \leq c(u,v)$
		\item $f(u,v) = -f(u,v)$
		\item für alle $u,v \notin \{s,t\}$ ist der Eingangsfluss gleich dem Ausgangsfluss
	\end{itemize}
	\textbf{Definition} (Restnetzwerk)\\
	Gegeben sei ein Netzwerk $G$. Die Restkapazität ist definiert als \[c_f(u,v) = c(u,v) - f(u,c)\]
	Die Kantenmenge ist gegeben als \[E_f = \{uv \in V\times V: \; c_f(u,v) > 0\}\]
	\textbf{Definition} (Erweiterungspfad)\\
	Das ist ein kreisfreier Pfad $P$ im Restnetzwerk $G_f$. Die Restkapazität des Pfades ist \[c_f(P) = \min\{c_f(u,v): \; uv \in E(P)\}\]
	Man kann den Fluss entlang von $P$ um $c_f(P)$ erhöhen.\\
	\textbf{Ford-Fulkerson}\\
	Dieser Algorithmus erhöht den Fluss nach der Methode der Erweiterungspfade.
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Ford-Fulkerson}
	\begin{algorithmic}[1]
		\ForAll{$(u,v) \in E$}
		\State $f(u,v) = 0$, $f(v,u) = 0$
		\EndFor
		\Repeat
		\State Berechne das Restnetzwerk $G_f$
		\State Suche einen Erweiterungspfad $P$ in $G_f$
		\State $c_f(P) = \min\{c_d(u,v): \; uv \in E(P)\}$
		\State $g(u,v) = \begin{cases}
			c_f(P),& uv \in E(P)\\
			-c_f(P),& vu \in E(P)\\
			0,& \text{sonst}
		\end{cases}$
		\State $f = f+g$
		\Until{es gibt keinen Pfad von $s$ nach $t$ in $G_f$}
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\subsection{Min-Cut, Max-Flow}
	\textbf{Definition}\\
	Gegeben sein ein Netzwerk wie immer. Ein Schnitt ist eine Zerlegung der Knotenmenge $V$ in disjunkte Teilmengen $A,B$, sodass $s \in A$, $t \in B$. Die Kapazität des Schnittes ist die Summer aller Kapazitäten von Kanten, die zwischen $A$ und $B$ laufen.\\
	\textbf{Theorem}\\
	Folgende Aussagen sind äquivalent: \begin{enumerate}
		\item $f$ ist maximal
		\item $G_f$ enthält keinen Erweiterungspfad
		\item $\left|f\right| = c(A,B)$ für einen Schnitt in $G$.
	\end{enumerate}
	Die Wahl des Erweiterungspfades im Ford-Fulkerson-Algorithmus ist nicht weiter eingeschränkt. Durch ungeschicktes Wählen, kann die Laufzeit in Abhängigkeit des maximalen Flusses wachsen. Dann ist die Komplexität $\mathcal{O}(\left|f\right|\cdot \left|E\right|)$. Die Edmonds-Kapr Strategie ist es, immer einen kürzesten Erweiterungspfad auszuwählen. Dadurch kann eine Laufzeit von $\mathcal{O}(\left|V\right|\cdot \left|E\right|^2)$ erreicht werden.\
	\subsection{Maximales Matching}
	Das Problem kann gelöst werden, indem eine Quelle und eine Senke hinzugefügt werden und auf dem Graphen das ungewichtete Netzwerk-Fluss Problem mit Ford-Fulkerson löst. Aufgrund der Natur von bipartiten Graphen erhält man eine Laufzeit von nur $\mathcal{O}(n\cdot m)$.\\
	\textbf{Satz}\\
	Satz von Hall: es gibt ein Matching der Größe $\left|V_1\right|$ genau dann, wenn die Nachbarschaft aller Teilmengen von $V_1$ in $V_2$ mindestens so groß ist.
	\section{Algebraische und Zahlentheoretische Algorithmen}
	\subsection{Multiplikation großer Zahlen}
	Die naive Methode hat Komplexität $\Theta(n^2)$. Wir führen die Methode von Karatsuba und Ofman ein:\\
	Multipliziere $x$ und $y$. Man zerlegt sie in zwei gleich lange Teile. Man multipliziert die beiden Teile und summiert sie. Dieses Verfahren kann rekursiv implementiert werden. Das Master Theorem liefert nun eine Laufzeit von $\Theta(n^2)$. Das heißt im allgemeinen Fall ist diese Methode nicht besser als der naive Algorithmus!\\
	Schreibt man den gemischten Term nun aber um $x_1y_2 = x_0y_0 + x_0y_ + (x_1-x_0)(y_0-y_1)$. Dann erreicht man eine Laufzeit von $\Theta(n^{\log_2 n})$.
	\subsection{Schnelle Matrixmultiplikation}
	Die schnelle Matrixmultiplikation verfolgt einen ähnlichen Ansatz, indem die Matrizen geviertelt werden. Dann können die Einträge der Produktmatrix in Rechnungen auf den Teilmatrizen berechnet werden. Genauso können die Teilmatrizen wieder zerlegen und erhalten ein Divide and Conquer Verfahren: 
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Mat-Prod($n,A,B$)}
	\begin{algorithmic}[1]
		\If{$n$ = 1}
		\State \textbf{return} $A\cdot B$
		\Else
		\State Teile in vier Teile auf und berechne \texttt{Mat-Prod($n/2, A, B)$}
		\EndIf
		\State \textbf{return} $A\cdot B$
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Die Laufzeit ist dann beschränkt durch $\Theta(n^{\log 7})$. Allerdings ist diese Methode mit einem großen Vorfaktor behaftet und ist daher nur bei großen $n$ tatsächlich besser als der naive Algorithmus.
	\subsection{RSA-Kryptografie}
	Alice will Bob eine Nachricht senden. Sie verwendet dazu einen öffentlich zugänglichen Schlüssel $(e,n)$. Bob verwendet seinen privaten Schlüssel $d$ um die Nachricht wieder zu entschlüsseln.\\
	\textbf{Schlüsselgenerierung}\\
	Man wähle zwei Primzahlen $p \neq q$ und berechnet $n = p \cdot q$. Wann wähle nun eine zu $(p-1)(q-1)$ teilerfremde Zahl $e$. Dann ist $(e,n)$ der öffentliche Schlüssel. $d$ ist ein multiplikativ inverses Element zu $e$ modulo $(p-1)(q-1)$.\\
	Alice kennt $(e,n)$ und verschlüsselt ihre Nachricht $m$, eine natürliche Zahl. Sie berechnet $m^e \mod n$ und erhält $c$. Diese verschlüsselte Nachricht wird an Bob geschickt. Bob entschlüsselt diese Nachricht wieder, indem er $c^d \mod n$ berechnet. Das Ergebnis ist wieder $m$, das ist eine Konsequenz des folgenden Satzes: \\
	\textbf{Satz}\\
	$m \equiv (m^e)^d \mod n$.\\
	\subsection{Euklidischer Algorithmus}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{Euklid(a,b)}
	\begin{algorithmic}[1]
		\If{$b = 0$}
		\State \textbf{return} $a$
		\Else
		\State \textbf{return} \texttt{Euklid($b, a \mod b$)}
		\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Der euklidische Algorithmus hat eine Bitkomplexität von $\mathcal{O}(n^2)$.\\
	\subsection{prime Restklassengruppe}
	Zunächst wird gezeigt, dass der größte gemeinsame Teiler von $a$ und $b$ eine Linearkombination der beiden ist. \[ggT(a,b) = \min(\{a\cdot x + b \cdot y \;|\; x,y\in \mathbb{Z}\} \cap \mathbb{N}_+)\]
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{EE(a,b)}
	\begin{algorithmic}[1]
		\If{$b = 0$}
		\State \textbf{return} $(a,1,0)$
		\Else
		\State $(d',x',y') =$ \texttt{Euklid($b, a \mod b$)}
		\State $d = d'$, $x = y'$, $y = x'-(a \div b)\cdot y'$
		\State \textbf{return} $(d,x,y)$
		\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Die Komplexität ändert sich nicht gegenüber des Euklidischen Algorithmus. Will man nun das multiplikative Inverse von $a \in (\mathbb{Z}/n\mathbb{Z})^*$, dann gilt $ggT(a,n) = 1$ und $1 = ax+ny \Leftrightarrow 1-ny = ax = 1 \mod n$. Wendet man den erweiterten Euklidischen Algorithmus auf $a$ und $n$ an, findet man also $x$, das inverse zu $a$.\\
	\subsection{Berechnen der modularen Exponentiation $a^b \mod n$}
	$a,b,n$ seien Zahlen mit höchstens $m$ Bits. Die naive Methode der Berechnung liefert eine Bitkomplexität von $\mathcal{O}(m^22^m)$, weil eine Multiplikation Bitkomplexität $\mathcal{O}(m^2)$ hat. Besser wird es, wenn man $b$ zunächst in binäre Form überführt: $b = (b_k...b_1b_0)_2$. Durch wiederholtes Quadrieren (und Multiplikation mit $a$) falls $b_i = 1$, kann man nun leichter die Potenz bestimmen.\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{ModExp($a,b,n$)}
	\begin{algorithmic}[1]
		\State $b = (b_k...b_1b_0)_2$
		\State $d \gets 1$
		\For{$i=k$ \textbf{to} 0}
		\State $d \gets (d\cdot d) \mod n$
		\If{$b_i = 1$}
		\State $d \gets (d\cdot a) \mod n$
		\EndIf
		\EndFor
		\State \textbf{return} d
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\section{Polynommultiplikation}
	Wir beschreiben zwei Polynome $a(x)$ und $b(x)$ als $a = (a_0,...,a_{n-1})$ und $(b_0,...,b_{n-1})$ wobei der Grad als gleich angenommen werden kann, da sonst mit $0$ aufgefüllt werden kann. Genau so können wir annehmen, dass $n$ eine Zweierpotenz ist. Die naive Implementierung mittels Faltung erreicht $\mathcal{O}(n^2)$. Mann kann die Polynome aber genauso an $2n-1$ Punkten darstellen, da das Produkt Polynom damit eindeutig festgelegt ist. Diese Multiplikation von $2n-1$ Punkten hat demnach eine Laufzeit von $\mathcal{O}(n)$. Die Implementierung mittels Horner-Schema und die erneute Interpolation des Produktes erfordern aber beide $\mathcal{O}(n^2)$, man gewinnt also nichts.
	\subsection{Diskrete Fourier Transformation}
	Die diskrete Fourier Transformation wertet ein Polynom gegeben durch $(a_0,...,a_{N-1})$ an den $N$-ten Einheitswurzeln aus. Wir halten $x_{k,N} = e^{i k(2\pi/N)}$ fest.\\
	Zu einem Koeffizientenvektor $a$ berechnen wir die Auswertung an den $N$-ten Einheitswurzeln und erhalten \[y_k = A(x_{k,N}) = \sum_{j=0}^{N-1} a_j x_{k,N}^j = \sum_{j=0}^{N-1}a_j e^{ijk(2\pi/N)}\]
	Den Vektor $y = (y_0,...,y_{N-1})$ bezeichnen wir als diskrete Fourier Transformierte.\\
	Ein Polynom $A$ sei gegeben. Dann kann man in $A^e$ und $A^o$ mit geraden bzw. ungeraden Koeffizienten zerlegen. Die Koeffizientenvektoren $a^e$ und $a^o$ sind analog gegeben. Bemerke aber, dass \begin{align*}
		A^e(x) &= \sum_{j=0}^{\frac{N}{2}-1} a_{2k}x^k\\
		A^o(x) &= \sum_{j=0}^{\frac{N}{2}-1} a_{2k+1}x^k
	\end{align*}
	Dann folgt $A(x) = A^e(x^2) + x\cdot A^o(x^2)$.

	Wenn $x$ eine $N$-te Einheitswurzel ist, dann ist $x^2$ eine $\frac{N}{2}$-te Einheitswurzel. Die Auswertung des Polynoms kann also auf die Auswertung von $A^e$ und $A^o$ zurückgeführt werden.
	\subsection{FFT Algorithmus}
	Gegeben sei $A$ als Vektor $a$. Diesen zerlegen wir in $a^e$ und $a^e$. Wir wenden den FFT Algorithmus rekursiv auf diese beiden Vektoren an. Wir erhalten die Fourier Transformierten $y^e$ und $y^o$. Die finale Transformierte $y$ erhält man schlussendlich durch \[y_k = y_k^e + x_{k,N}\cdot y_k^o\] für $k \leq \frac{N}{2}-1$. Sonst bekommt man \[y_{\frac{N}{2}+k} = y_k^e - x_{k,N}\cdot y_k^o\]
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\texttt{FFT($N,a$)}
	\begin{algorithmic}[1]
		\If{$N = 1$}
		\Return $a_0$
		\Else
		\State $a^e \gets (a_0,a_2,...,a_{N-2})$
		\State $a^o \gets (a_1,a_3,...,a_{N-1})$
		\State $y^e \gets$ FFT($\frac{N}{2},a^e$)
		\State $y^o \gets$ FFT($\frac{N}{2},a^o$)
		\State $z \gets e^{i(2\pi(N))}$
		\State $x \gets 1$
		\For{$k=0$ to $\frac{N}{2}-1$}
		\State $y_k \gets y_k^e + x\cdot y_k^o$
		\State $y_{\frac{N}{2}+k} \gets y_k^e - x\cdot y_k^o$
		\State $x \gets z\cdot x$
		\EndFor
		\EndIf\\
		\Return $(y_0,...,y_{N-1})$ 
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Mit Master Theorem findet man eine Komplexität von $\Theta(N\log N)$.\\
	
	Zur Interpolation fehlt die inverse FFT, die ebenfalls in $\mathcal{O}(n\cdot \log n)$ gelingen sollte. Die Fourier Transformation lässt sich als Matrix-Vektor Multiplikation \[y = A \cdot a\] interpretieren. Dabei ist die Matrix $A$ die Vandermonde Matrix mit $(A)_{kj} = z^{jk}$. Diese Matrix ist invertierbar, damit ist die Rückrichtung leicht gegeben durch $a = A^{-1}y$. Es kann leicht gezeigt werden, dass $(A^{-1})_{jk} = \frac{z^{-jk}}{N}$. Mittels der inversen Matrixmultiplikation benötigt man aber $\mathcal{O}(n^2)$ Zeit. Wenn man aber im FFT-Algorithmus \begin{enumerate}
		\item die Rollen von $a$ und $y$ vertauscht
		\item $z$ durch $\overline{z}$ ersetzt
		\item jede Komponente des Ergebnis durch $N$ teilt
	\end{enumerate}
	dann erhält man die Rückrichtung in $\mathcal{O}(n\cdot \log n)$ Zeit.\\
	
	Für zwei Polynome $a$ und $b$ gegeben als Koeffizientenvektoren kann man das Produktpolynom also bestimmen durch $FFT^{-1}(FFT(a)\cdot FFT(b))$ in $\mathcal{O}(n\cdot \log n)$ Zeit.
	
	\section{Suchbäume}
	Suchbäume unterstützen die meisten Operationen auf Mengen in mittlerer Komplexität von $\mathcal{O}(n\cdot \log n)$ und im worst-case in $\mathcal{O}(n)$ Zeit.\\
	Beispiel Binärer Suchbaum. Sei $T$ ein Binärbaum. Man bezeichne die Wurzel als Schlüssel $x$. Damit die Suchbaum Eigenschaft gilt, muss $T$ erfüllen: \begin{itemize}
		\item Für jeden Teilbaum auf der linken Seite ist der Schlüssel höchstens $x$
		\item im rechten umgekehrt.
	\end{itemize}
	Der Suchalgorithmus, die Suche nach Minimum und die nach Maximum werden durch diese Struktur leicht möglich.
	\subsection{AVL Bäume}
	Ein binärer Suchbaum heißt AVL-Baum, wenn sich die Höhe des linken und rechten Teilbaums um maximal 1 unterscheiden. Der Balance-Grad ist definiert als Höhe(rechter Teilbaum)-Höhe(linker Teilbaum), $bal(T) = h(T_r)-h(T_l) \in \{-1,0,1\}$. AVL-Bäume können nicht degenerieren, also in eine Liste entarten. Ein AVL Baum der Höhe hat mindestens $F_{h+2}$ Blätter, wobei $F_{h+2}$ die $h+2$-te Fibonacci-Zahl ist. Die Höhe eines AVL-Baumes ist proportional zu $\log n$.\\
	
	Das Einfügen eines Elementes in einem AVL-Baum kann die AVL-Eigenschaft verletzen. Daher muss man eine große Fallunterscheidung vornehmen. Der Balance-Grad muss bei jedem Knoten mitgeführt werden, um eine effiziente Implementierung zu ermöglichen.\\
	Die Implementierung muss mehrere Fallunterscheidungen treffen und benötigt dafür eine rekursive Hilfsmethode. Die Laufzeit kann aber mit $\mathcal{O}(n\cdot \log n)$ beschränkt werden. Das Löschen aus einem AVL-Baum kann mit einem ähnlichen Algorithmus mit den selben Fällen implementiert werden.
	\subsection{Branch and Bound}
	Hierbei handelt es sich um eine Methode zur Lösung von Optimierungsproblemen, bei denen keine effizienten Verfahren bekannt sind. Es werden folgende Voraussetzungen gefordert: \begin{enumerate}
		\item Lösungsraum ist strukturiert
		\item Baumstruktur: \begin{enumerate}
			\item Leere Lösung ist Wurzel
			\item Kindknoten: jeweils nächste Erweiterung der vorherigen Teillösung
		\end{enumerate}
	\end{enumerate}
	Eine leichte Anwendung ist das Travelling Salesman Problem.\\
	
	Im Branch Schritt werden neue Kindknoten erzeugt. Dies geschieht analog zur Breitensuche. Jeder Teillösung wird eine obere Schranke zugeordnet. Im TSP Beispiel wird immer die Teillösung mit minimalem Bound gewählt. Hier bietet sich eine Priority Queue für die Verwaltung der Bounds an. Als Queue könnte zum Beispiel ein Min-Heap verwendet werden.\\
	
	Der Bound ist anschaulich eine Schranke für die Bewertung der Lösungen, die sich aus dieser Teillösung ergeben. Hat eine Teillösung den Bound $b$, dann hat keine Lösung die sich dadurch ableiten lässt einen Wert, der besser als $b$ ist.\\
	\underline{Vorgehensweise}\\
	Berechne Bound $b$ der leeren Lösung $v$ und füge $(v,b)$ in den Heap ein\\
	Wiederhole: Entferne $(v,b)$ mit minimalem Bound aus dem Heap.\\
	Falls $v$ vollständige Lösung ist und $v \leq$ alle anderen Bounds, gib $v$ aus.\\
	Sonst: Erweitere $v$ (Branch). Für jede Erweiterung $w$ von $v$ bestimme $b_w$. Falls $b_w \geq b$, füge $(w,b_w)$ in den Heap ein. Sonst füge $(w,b)$ ein.
	\subsection{Spielbäume}
	Fragestellung: Wie bestimmt man den optimalen Zug bei einem strategischen Spiel?\\
	Es ist in der Regel nicht möglich, alle Spielzustände durchzurechnen (z.B. Schach), daher werden üblicherweise nur die nächsten Stufen simuliert.
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\underline{$Max(x)$}
	\begin{algorithmic}[1]
		\If{$x$ ist Blatt}
		\Return Bewertung von $x$
		\Else
		\State Seien $x_1,...,x_k$ die Kinder von $x$
		\State $w \gets -\infty$
		\For{$i = 1$ to $k$}
		\State $v \gets Min(x_i)$
		\If{$v>w$}
		\State $w \gets v$
		\EndIf
		\EndFor\\
		\Return $w$
		\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Die Prozedur $Min(x)$ wird analog implementiert.
	Die beiden Funktionen modellieren einen sogenannten MiniMax-Baum.\\
	Wir führen nun sogenannte AlphaBeta-Bäume ein, die es erlauben, dass ein Spielbaum nicht vollständig untersucht werden muss:\\
	
	Die Maximumsbestimmung kann in einer tieferen Ebene beendet werden, wenn das vorläufige Maximum mindestens so groß wie das vorläufige Minimum einer höheren Ebene ist. Für die Implementierung müssen wir also das vorläufig erreichte Maximum und Minimum in die rekursiven Prozeduren übergeben. Die beiden Werte werden Alpha und Beta genannt. 
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\underline{$AlphaBetaMax(x, \alpha, \beta)$}
	\begin{algorithmic}[1]
		\If{$x$ ist Blatt}
		\Return Bewertung von $x$
		\Else
		\State Seien $x_1,...,x_k$ die Kinder von $x$
		\State $w \gets \alpha$
		\For{$i = 1$ to $k$}
		\State $v \gets AlphaBetaMin(x_i, w, \beta)$
		\If{$v>w$}
		\State $w \gets v$
		\EndIf
		\If{$w\geq \beta$}
		\Return $w$
		\EndIf
		\EndFor\\
		\Return $w$
		\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Angenommen der Spielbaum habe Verzweigungsgrad $b$. In Tiefe $t$ hat der Baum also $b^t$ Blätter. Der mittlere Verzweigungsgrad von Alpha-Beta-Bäumen ist der Verzweigungsgrad ungefähr $\tilde{b} = \alpha_b/(1-\alpha_b)$ wobei $\alpha_b$ die Lösung von $x^b+x=1$ ist. Für Werte bis ca. $b = 1000$ entspricht das ungefähr der Funktion $0.925\cdot b^{0.747}$. Für Schach haben wir einen mittleren Verzweigungsgrad von 35. In der Tiefe $t$ erhält man also $35^t$ Blätter im MiniMax-Baum. In der Alpha-Beta-Variante ist der Grad auf ungefähr 13 beschränkt. Bei gleichem Speicheraufwand kann im Alpha-Beta-Baum also eine ungefähre Tiefe von $t' = 1.38\cdot t$ erreicht werden.\\
	
	Die beiden Funktionen Max und Min können auch in nur einer Prozedur abgearbeitet werden, denn $\max(x,y) = -\min(-x,-y)$. Wenn man sich außerdem eine Methode überlegen könnte, bei der die besseren Kinder in jedem Iterationsschritt als erstes betrachtet werden, könnte man noch weitere Verbesserungen erreichen.
	\section{String Matching} 
	Das Problem des String Matchings sind alle Vorkommen eines Musters in einem Text gesucht. Der naive Algorithmus geht alle Positionen durch und vergleicht die Buchstaben des Musters schrittweise mit dem Text. Sobald ein Buchstabe nicht mit dem Text übereinstimmt, wird die nächste Position untersucht. Habe der Text Länge $m$ und das Muster $n$ dann hat die naive Methode eine Laufzeit von $\mathcal{O}(n\cdot m)$.
	\subsection{Matching durch Hashing}
	An jeder Stelle soll der Hashwert des Musters mit dem des Teilstrings verglichen werden. Nur wenn die Hahswerte übereinstimmen, dann werden die tatsächlichen Strings verglichen.
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Man betrachte einen Text $T$ der Länge $n$, ein Muster $P$ der Länge $m$ und die Hashwerte $p$ für $P$ und $t$ für den Teilstring $T[1,...,m]$.\\
	\underline{Rabin-Karp-Matcher}
	\begin{algorithmic}[1]
		\For{$s = 0$ to $n-m$}
		\If{$p = t$}
		\If{Test($T,P,s$)}
		\Return $s$
		\EndIf
		\EndIf
		\If{$s<n-m$}
		\State aktualisiere $t$
		\EndIf
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Die Test-Funktion überprüft in linearer Zeit, ob zwei Strings gleich sind. Für die Komplexität relevant sind also die Länge des Textes und des Musters und die Anzahl der Kollisionen der gewählten Hashfunktion. Wir wählen als Hashfunktion die Kongruenzmethode $h(x) \equiv x \mod q$ mit Tabellengröße $q$. Die $r$ Buchstaben des Alphabets werden durchnummeriert, und im $r$-adischen System betrachtet, damit jedem Buchstaben eine Ziffer zugeordnet werden kann und wir mit ihnen rechnen können.\\
	Die Aktualisierung des Hashwertes wird wie folgt implementiert:
	Subtrahiere vom vorherigen Teilstring das vorderste Bit mit der jeweiligen Magnitude (also die Stelle als Zahl). Multipliziere den verbliebenen String mit $\left| \Sigma\right|$ und addiere das neue Bit. Berechne die dabei entstandene Zahl wieder modulo $p$ in der Kongruenzmethode. Dadurch können die neuen Hashwerte in $\mathcal{O}(1)$ berechnet werden.\\
	Im worst-case kann man mit einer Komplexität von $\mathcal{O}(n\cdot m)$ rechnen. Im average-case kann eine Laufzeit von $\mathcal{O}(n+m)$ bewiesen werden.\\
	
	Die naive Methode bietet gute Möglichkeiten zur Verbesserung mittels endlicher Automaten an. Wird in einem Zustand ein falscher Buchstaben eingelesen, wird im endlichen Automaten zu einem früheren Zustand zurückgegangen. Hierbei muss die Länge des vielleicht richtigen Präfixes berücksichtigt werden. Nun kann man sich leicht einen Algorithmus überlegen:\\
	Zum Pattern wird der endliche Automat aufgebaut. Der Text wird durchlaufen und es wird für den $i$-ten Buchstaben die $i$-te Übergangsfunktion gewählt. Würde der Automat terminieren, so ist der String gefunden und der Algorithmus gibt einen dementsprechenden Output aus.\\
	
	Wesentlich effizienter ist der Knuth-Morris-Pratt-Algorithmus. Der KMP-Algorithmus verschiebt bei einem Mismatch das Muster um einen Shift, der den längsten Teilstring, den Prä- und Suffix gemeinsam haben, berücksichtigt. Dafür berechnet man eine Verschiebetabelle für jeden Buchstaben im Muster. Dabei ist \[V[j] = \max\left\{k<j | P[1,...,k] = P[j-k+1,...,j]\right\}\]
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Wir nehmen im Folgenden die Verschiebetabelle $V$ als gegeben an.\\
	\underline{KMP-Algorithmus}
	\begin{algorithmic}[1]
		\State $j \gets 0$
		\For{$i=1$ to $n$}
		\While{$j>0$ and $P[j+1] \neq T[i]$}
		\State $j \gets V[j]$
		\EndWhile
		\If{$P[j+1] = T[i]$}
		\State $j \gets j + 1$
		\EndIf
		\If{$j = m$}
		\State print: pattern occurs with shift $i-m$
		\State $j \gets V[j]$
		\EndIf
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Mittels einer sogenannten amortisierten Analyse kann gezeigt werden, dass der Algorithmus sogar eine worst-case Komplexität von $\mathcal{O}(n)$ besitzt.
	\subsection{Boyer-Moore-Algorithmus}
	In diesem Fall, wird das Muster von rechts nach links verglichen. Es wird eine Verschiebetabelle erstellt und bei einem Mismatch um $n-V[m]$ verschoben. Es gibt zwei Fälle: \subsubsection{Bad-Character-Heuristik}
	Sei $\omega$ der Buchstabe des Mismatches im Text. Diese Heuristik verschiebt das Muster bis zur maximalen Stelle, an der ebenfalls ein $\omega$ steht. Vorsicht: steht das Zeichen rechts von dem aktuell verglichenen Teilmuster, wird ein negativer Shift (also in die falsche Richtung) vorgeschlagen. 
	\subsubsection{Good-Suffix-Heuristik}
	Es wird so lange nach rechts verschoben, bis wieder ein Teilstring des Musters unter dem gematchten Substring des Textes liegt. Das ist der Good-Suffix des Musters.\\
	Mit kleinen Veränderungen kann man in der Implementierung eine Laufzeit von $\mathcal{O}(n)$ erreichen. In der Anwendung ist das der schnellste Stringmatcher.
	\subsection{Boyer-Moore-Horspool}
	Das ist eine Erweiterung des BM-Algorithmus, die nur auf der Bad-Character Heuristik basiert. Hierbei werden negative Shifts effizient vermieden.\\
	Man berechne eine Tabelle $\lambda$. Diese Tabelle sagt, kommt ein Buchstabe im vorderen Teil des Strings nicht mehr vor, kann über die gesamte Länge des Musters verschoben werden. Die Tabelle kann effizient wie folgt berechnet werden:
	\par\noindent\rule{\textwidth}{0.4pt}\\
	\underline{Berechne $\lambda$}
	\begin{algorithmic}[1]
		\For{$c \in \Sigma$}
		\State $\lambda(c) = m$
		\EndFor
		\For{$j = 1$ to $m-1$}
		\State $\lambda(P[j]) = m-1$
		\EndFor
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}\\
	Im BMH-Algorithmus wird der Shift $s$ immer erhöht auf $s+\lambda(T[s+m])$.
	\subsection{Suffix-Bäume}
	Unsere beste Möglichkeit zur exakten String-Suche (also dem Finden aller Matchings) ist momentan ein String-Matcher in bestenfalls $\mathcal{O}(n)$ Zeit. Wri werden Methoden zum Pre-Processing des Textes zeigen, mit denen sich die Laufzeit wesentlich verringert. Ein Suffix-Baum ist ein Baum mit $n$ Blättern mit folgenden Eigenschaften \begin{itemize}
		\item Jeder innere Knoten außer der Wurzel hat mindestens zwei Kinder
		\item Jede Kante hat als Label einen nicht-leeren Substring von $T$
		\item Alle Kanten, die einen beliebigen Knoten verlassen, haben einen anderen ersten Buchstaben
		\item Wenn man die Labels des Weges der Wurzel bis zum $k$-ten Blatt aneinanderhängt, erhält man den $k$-ten Teilstring. 
	\end{itemize}
	Ein Problem, kann sich ergeben, wenn ein Suffix Präfix eines anderen Suffixes ist. Das kann verhindert werden, indem man jeden Suffix implizit mit \$ enden lässt. Der Speicheraufwand ist mit $\mathcal{O}(n^2)$ quadratisch, sollte aber nur linearen Platz brauchen. Das Problem dabei ist, dass jede Kante als Label einen Teilstring besitzt. Speichert man daher nur Anfangs- und Endbit des Teilstrings, erhält man linearen Speicheraufwand.\\
	
	Beschäftigen wir uns nun mit der Konstruktion eines Suffixbaums: Die leichteste und nicht effizienteste (weil quadratische) Algorithmus erstellt zunächst eine Kante für den ersten Suffix, also das ganze Wort. Für alle folgenden Teilworte wird überprüft, ob es bereits eine Kante mit selbem Anfangsbuchstaben gibt. Wenn schon, wird an solchen Kanten entlang gegangen und überprüft, ob die folgenden Buchstaben wieder gleich sind. Die Kante mit dem längsten solchen Match wird geteilt und der nicht gematchte Teil des neuen Suffix wird von dem neuen Knoten entlang einer Kante bis zu einem neuen Blatt geschrieben. Macht man eine Tiefensuche in diesem Baum, erhält man eine lexikografisch Sortierte Reihe aller Suffixe.\\
	Sucht man nun nach einem Muster $P$ in $T$, so werden dieselben Schritte, wie beim Erstellen des Baumes durchgeführt. Man sucht zunächst nach einer Kante, die mit dem selben Buchstaben beginnt. Dann wird an diesen Kanten entlang weiter gesucht usw.
	\subsection{Suffix-Arrays}
	Wir ergänzen wieder ein \$ am Ende des Strings, um die oben beschriebene Problematik zu umgehen. Wir versuchen, alle Suffixe des Strings lexikografisch zu sortieren. Mit zum Beispiel Mergesort, benötigt das $\mathcal{O}(n^2\log n)$ Zeit, denn jeder Vergleich von zwei Strings benötigt lineare Zeit. Die Idee ist wieder, statt der Teilstrings nur die Indizes zu speichern, an denen der jeweilige Substring beginnt. Das sortierte Array heißt Suffix-Array. Wie wird in so einem Array nun gesucht?\\
	Die Idee ist, binäre Suche zu verwenden. Man suche binär nach Indizes, an denen der erste Buchstabe unseres Patterns mit dem Buchstaben des Textes an diesem Index übereinstimmt. Mit $\log n$ Schritten, kann die linke Grenze dieser Buchstaben gefunden werden. Auf die selbe Weise, findet man die rechte Grenze. Dann müssen lediglich die Suffixe in diesem Teilarray untersucht werden. Man kann alle $j$ Instanzen eines Teilstrings der Länge $m$ in einem Text der Länge $n$ in $\mathcal{O}(m \log n + j)$ Zeit finden.
\end{document}