\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{algpseudocode}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{Alter Zusammenfassung}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\section{Analyse von Algorithmen}
	\subsection{asymptotische Notation}
	\textbf{Definition} ($\mathcal{O}$-Notation)
	\[\mathcal{O}(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: g(n) \leq c\cdot f(n)\}\]
	\textbf{Definition} ($\Omega$-Notation)
	\[\Omega(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: f(n) \leq c\cdot g(n)\}\]
	\textbf{Definition} ($\Theta$-Notation) \[\Theta(f(n)) = \mathcal{O}(f(n)) \cap \Omega(f(n))\]

	\subsection{worst-case und average-case}
	\textbf{Definition} (worst-case)\\
	Die worst-case Zeit ist die für eine fixe Länge $n$ des Inputs schlechteste Laufzeit eines Algorithmus $A$. \[wc-time_A(n) := \max_{x: \left|x\right| = n} time_A(x)\]
	\textbf{Definition} (average-case)\\
	Bei der average-case Analyse wird angenommen, dass die Eingaben der Länge $n$ gleichverteilt sind. Damit ist die Laufzeit eine Zufallsvariable $T_{A,n}$. \[av-time_{A}(n) := \mathbb{E}[T_{A,n}]\] Mit der Definition des Erwartungswertes der Gleichverteilung also \[av-time_A(n) = \frac{1}{\left|\{x: \left|x\right| = n\}\right|} \cdot \sum_{x: \left|x\right| = n} time_A(n)\]
	\subsection{Abschätzungen}
	Die harmonische Reihe der ersten $n$ Glieder lässt sich abschätzen durch \[H_n = \sum_{i=1}^{n} \frac{1}{i} \leq \ln(n)+1\]
	\textbf{Definition} (Stirling Formel)
	\[n! \sim \sqrt{2\pi n} (\frac{n}{e})^n\]
	wobei $f(n) \sim g(n) \Leftrightarrow \lim_{n\to\infty} \frac{f(n)}{g(n)} = 1$. Daraus folgt auch \[\log(n!) \sim n\log(n) - n\log(e) + \log(\sqrt{2\pi n})\]
	\subsection{Rekursionsgleichungen}
	Zum Beispiel kann die Fibonacci Folge durch $F(n) = \Omega(2^{n/2})$ von unten abgeschätzt werden, da $F(n) \geq 2F(n-1)$.\\
	\textbf{Behauptung}: $F(n) \geq ac^n$ für $a,c >0$.\\ Beweis durch Induktion liefert $c \leq \phi = \frac{\sqrt{5}+1}{2}$. Insgesamt folgt $F(n) = \Theta((\frac{\sqrt{5}+1}{2})^n)$.\\
	\textbf{Beispiel} (Divide and Conquer)\\
	Das zu lösende Problem wird in zwei Teile mit geringerer Größe geteilt. Die Lösung für ein Problem setzt sich aus der Lösung für alle Teilprobleme zusammen. Die Laufzeit ist durch $\mathcal{O}(n\log(n))$ beschränkt. Beispiele für Divide and Conquer Verfahren sind Quicksort, Mergesort oder Binäre Suche.\\
	Mergesort: eine Eingabefolge $a_n$ wird in zwei gleich große Teile gespalten und die beiden Hälften rekursiv sortiert. Im Basisfall werden die beiden Hälften kombiniert und sortiert (merging).\newpage
	\textbf{Algorithmus}\\
	\texttt{INPUT: $x$\\
	IF $\left|x\right|\leq 1$\\
	\indent Berechne die Lösung $L$ für $x$ direkt\\
	ELSE: $x = x_1 \cup x_2$\\
	\indent $L_1$ = DivideAndConquer($x_1$)\\
	\indent $L_2$ = DivideAndConquer($x_2$)\\
	\indent Berechne $L$ aus $L_1$ und $L_2$\\
	RETURN $L$}\\
	Für den Aufwand $f$ folgt $f(n) = 2\cdot f(\frac{n}{2}) + c\cdot n + d$.	\\
	\textbf{Behauptung}: $f(n) = \alpha + \beta n +\delta n \log(n)$\\
	Der Beweis folgt durch elementares Nachrechnen. Es folgt durch Koeffizientenvergleich $\alpha = -d$ und $\delta = c$. Für den Mergesort erhält man zum Beispiel $f(n) = n\log(n) - n +1$.\\
	Die Berechnung kann auch durch Iterationsmethode berechnet werden. Dabei wird die Rekursionsgleichung immer wieder in sich selbst eingesetzt. Diese Einsetzung kann $log(n)$ oft geschehen, dann ist der Basisfall 1 erreicht. Mittels der $k$-ten Partialsumme der geometrischen Reihe, kann die Laufzeit bestimmt werden.\\
	\textbf{Beispiel} (Andere Rekursionen)\\
	Zum Beispiel für Probleme der Form \[f(n) = 2f(\sqrt{n}) + \log(n)\] hilft Iteration nicht weiter. Aber man kann sich durch Variablensubstitution behelfen. Mit $n = 2^m$ erhält man \[f(2^m) = 2f(2^{m/2}) + m\] Mit $f(2^m) = g(m)$ also \[g(m) = 2g(\frac{m}{2}) + m\]
	\textbf{Vereinfachung}:\begin{itemize}
		\item ganzzahlige Rekursionen $f(n) = f(\lfloor n\rfloor) + f(\lceil n \rceil)$ können ignoriert werden und von der Form $f(n) = 2f(\frac{n}{2})$ geschrieben werden.
		\item Ignorieren der Anfangswertbedingung
		\item Nur $\Omega$ und $\mathcal{O}$ Notation bestimmen
		\item Abschätzungen, die nur für große $n$ gelten
		\item \underline{Vorsicht:} asymptotische Notation in der Rekursiongleichung kann zu Problemen führen. 
	\end{itemize}
	\subsection{Master Theorem}
	\textbf{Satz}\\
	Gegeben eine Rekursiongleichung der Form \[T(n) = \sum_{i=1}^m T(\alpha_i\cdot n) + \Theta(n^k)\] wobei $0 \leq \alpha_i \leq 1$, $m\geq 1$ und $k\geq 0$. Dann kann man $T(n)$ abschätzen durch \[T(n) = \begin{cases}
		\Theta(n^k), & \text{falls } \sum_{i=1}^m \alpha_i^k < 1\\
		\Theta(n^k \log n), & \text{falls } \sum_{i=1}^m \alpha_i^k = 1\\
		\Theta(n^c), & \text{sonst}
	\end{cases}\]
	$c$ im letzten Fall ergibt sich dabei durch Lösen der folgenden Gleichung: \[\sum_{i=1}^m \alpha_i^c = 1\] Wenn alle $\alpha_i$ gleich sind, ergibt sich der Spezialfall $c = \frac{-\ln m}{\ln \alpha}$\\
	\textbf{Selektionsalgorithmen}\\
	Ein Selektionsalgorithmus soll für eine Folge $a_n$ und ein $i$ feststellen, welches das $i$-kleinste Element von $a_n$ ist. Zum Beispiel für $i=n$ das Maximum, $i=1$ das Minimum und $i=\frac{n}{2}$ der Median. Der naive Ansatz, die Liste zuerst zu sortieren, kann aber laufzeittechnisch verbessert werden. Als Komplexitätsmaß wird die Vergleichskomplexität $V(n)$ herangezogen. Im worst-case muss jeder Sortieralgorithmus so viele Vergleiche machen, wie man benötigt, um $n!$ durch halbieren zu einer Zahl $\leq 1$ zu machen. Mit der Stirling Formel ergibt sich, dass jeder vergleichsbasierte Sortieralgorithmus eine worst-case Laufzeit von $\geq n\log n - \Theta(n)$ hat. Die Darstellung eines solches Verfahren kann mit einem Entscheidungsbaum geschehen.\\
	\underline{\textbf{Heap Sort}}\\
	Ein Heap ist ein Binärbaum mit folgenden beiden Eigenschaftem \begin{itemize}
		\item der Baum ist soweit es geht vollständig gefüllt, die unterste Stufe ist von links gefüllt.
		\item Die Knoten haben Zahlen. Die Kindknoten eines beliebigen Knotens haben Werte kleiner oder gleich des Wertes des Elternknotens.
	\end{itemize}
	Der HeapSort läuft in zwei Phasen: \begin{enumerate}
		\item die Zahlenfolge wird zu einem Heap gebaut
		\item der Heap wird abgebaut, indem die Wurzel entfernt wird und ein anderer Knoten an ihre Stelle gesetzt wird, sodass die Heap Eigenschaft weiter erfüllt ist.
	\end{enumerate}
	HeapSort ist ein Inplace-Sortierverfahren. Die worst-case-Laufzeit beträgt $\mathcal{O}(n\log n)$.\\
	\underline{\textbf{Quicksort}}\\
	Wir betrachten Quicksort, ein stabiles divide-and-conquer Verfahren.
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\State\texttt{PROCEDURE} Quicksort($a_1,...,a_n$)
		\If{$n=0$}
			\State RETURN
		\EndIf
		\State $x\gets a_1$
		\State $l \gets$ leere Folge
		\State $r \gets$ leere Folge
		\For{$i=2$ TO $n$}
			{\If{$a_i \leq x$}
				\State{append $a_i$ to $l$} 
			\Else
				\State {append $a_i$ to r}
			\EndIf}
		\EndFor
		\State RETURN (QuickSort(l),$x$,QuickSort(r))
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\underline{Eigenschaften} \begin{itemize}
		\item im schlechtesten Fall wird jedes Mal die größte (oder kleinste) Zahl als Pivotelement gewählt
		\item Dann hat die $i$-te Teilfolge Läge $n-i$
		\item im schlimmsten Fall also eine Laufzeit von $\Theta(n^2)$
		\item im best-case ist das Pivotelement der Median
		\item die $i$-te Teilfolge hat dann Länge $\frac{n}{2^i}$ 
		\item das bringt eine Laufzeit von $n\log n + \Theta(n)$
	\end{itemize}
	\subsection{Average-Case-Analyse von Quicksort}
	Annahme: die Eingabe Liste ist gleichverteilt und paarweise verschieden.\\
	Die Eingabefolge sein $a_n$ und die sortierte Folge $s_n$. Wir führen eine Zufallsvariable $X_{ij}$ ein, wobei $X_{ij} = 1$, falls beim Quicksort $s_i$ mit $s_j$ verglichen werden. Daher ist die mittlere Anzahl an Vergleichen gegeben durch \[V(n) = \sum_{i<j}\mathbb{E}[X_{ij}]\]
	Nun ist $\mathbb{E}[X_{ij}] = p_{ij} + 0\cdot (1-p_{ij}) = p_{ij}$ die Wahrscheinlichkeit, dass $s_i$ und $s_j$ verglichen werden. Man findet, dass $p_{ij} = \frac{2}{j-i+1}$. Insgesamt also \[V(n) = \sum_{i<j} \mathbb{E}[X_{ij}] = \sum_{i<j} \frac{2}{j-i+1}\] Durch elementares Umformen findet man $V(n) \leq 1,3863n \log_2 n$. Die obige Implementierung von Quicksort ist nicht in-place, denn es werden immer neue (Teil-)Folgen generiert.
	\section{Selektionsalgorithmen}
	Gegeben sei eine Folge von Zahlen. Es soll das $i$-kleinste Element gefunden werden für $i \in [n]$. Bei $i=1$ das Minimum, bei $i=n$ das Maximum, bei $i = \frac{n}{2}$ den Median.\\
	\subsection{Spezialfall Extrema (z.B. Maximum)} 
	Die naive Implementierung braucht lineare Zeit und genauer $V(n) = n-1$ Vergleiche. Diese Implementierung ist optimal, denn bei weniger als $n-1$ Vergleichen gibt es ein Element, das nie überprüft wurde.
	\subsection{Maximum und Minimum}
	Die naive Implementierung bestimmt die Extrema separat und benötigt daher $2n-2$ Vergleiche. Dieser Wert lässt sich verbessern:\\ Es wird immer ein Element mit seinem Nachfolger verglichen. Dabei gibt es $\frac{n}{2}$ größere und $\frac{n}{2}$ kleinere Elemente. Für diese Partition braucht man daher $\frac{n}{2}$ Vergleiche. Unter den kleineren kann das Minimum und unter den größeren das Maximum in jeweils $\frac{n}{2}-1$ Vergleichen bestimmt werden. Insgesamt folgt also $V(n) = \frac{3}{2}n-2$. Man kann zeigen, dass dieses Vorgehen optimal ist.
	\subsection{Selektion des $k$-kleinsten Elementes}
	Die Vorgehenweise ist angelegt an Quicksort. Folgende Prozedur löst das Problem:\\
	Select($a,l,r,i$) bestimmt das $i$-kleinste Element in der Teilfolge $a[l,r]$.\\
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\If{$i+l-1\notin \{l,...,r\}$} 
		\State return fail
		\EndIf
		\State Bestimmte Pivotelement $x$ aus $a[l,...,r]$
		\State Sei $m$ der Rang von $x$ in $a[l,...,r]$
		\State sortiere $a$ so um, dass $a[l+m-1]=x \land \begin{cases}
			a[j] < a[m+l-1] & \text{ für } l\leq j<m+l-1\\
			a[j] > a[m+l-1] & \text{ für } r \geq j >m+l-1 
		\end{cases}$
	\If{$i=m+l-1$}
	\State return $x$
	\EndIf
	\If{$i<m+l-1$}
	\State return Select($a,l,l+m-2,i$)
	\EndIf
	\If{$i>m+l-1$}
	\State return Select($a,l+m,r,i-m$)
	\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\textbf{Worst-Case-Komplexität}\\
	Analog zum Quicksort tritt der schlechteste Fall ein, wenn das Pivotelement am Rand liegt. Dann ist die Komplexität $\mathcal{O}(n^2)$.\\
	\textbf{Random Select}\\
	Angenommen das Pivotelement wird zufällig gemäß einer Gleichverteilung gewählt. Ist $x$ das $j$-kleinste Element so hat das linke Teilarray die Größe $j-1$ und das rechte $n-j$. Im schlimmsten Fall wird Select rekursiv mit dem größeren Teilarray aufgerufen. die Aufteilung benötigt $n-1$ Vergleiche. D.h. die mittlere Anzahl von Vergleichen ist rekursiv beschränkt durch $V(n) \leq (n-1) + \frac{2}{n} \sum_{j = n/2}^{n-1} V(j)$. Durch Einsetzen kann man diese Rekursionsungleichung lösen und findet, dass der average case linear ist.\\
	\textbf{Pivotelement geschickt wählen}\\
	Man organisiert die Liste in Fünfergruppen und sortiert diese (jeweils konstant mit genau 7 Vergleichen). In jeder Fünfergruppe wird der Median bestimmt und anschließend der Median dieser Mediane. Dieser wird als Pivotelement gesetzt. Damit hat sich die Anzahl der zu vergleichenden Elemente auf $\frac{4}{10}n$. Es ergibt sich die Rekursionsgleichung \[V(n) = \frac{7n}{5} + V(\frac{n}{5} + \frac{4n}{10}) + V(\frac{7n}{10})\] Mittels Mastertheorem lässt sich diese Gleichung lösen und man findet, dass die Prozedur mit dieser Pivotwahl linearen Aufwand hat.
\end{document}