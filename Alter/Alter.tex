\documentclass[a4paper, 12pt]{article}

\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath,amssymb}
\usepackage[explicit]{titlesec}
\usepackage{ulem}
\usepackage[onehalfspacing]{setspace}
\usepackage{algpseudocode}

\titleformat{\subsection}
{\small}{\thesubsection}{1em}{\uline{#1}}
\begin{document}
	\begin{titlepage} 
		\title{Alter Zusammenfassung}
		\clearpage\maketitle
		\thispagestyle{empty}
	\end{titlepage}
	\tableofcontents
	\section{Analyse von Algorithmen}
	\subsection{asymptotische Notation}
	\textbf{Definition} ($\mathcal{O}$-Notation)
	\[\mathcal{O}(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: g(n) \leq c\cdot f(n)\}\]
	\textbf{Definition} ($\Omega$-Notation)
	\[\Omega(f(n)) = \{g: \exists c>0\; \exists n_0 \; \forall n\geq n_0: f(n) \leq c\cdot g(n)\}\]
	\textbf{Definition} ($\Theta$-Notation) \[\Theta(f(n)) = \mathcal{O}(f(n)) \cap \Omega(f(n))\]

	\subsection{worst-case und average-case}
	\textbf{Definition} (worst-case)\\
	Die worst-case Zeit ist die für eine fixe Länge $n$ des Inputs schlechteste Laufzeit eines Algorithmus $A$. \[wc-time_A(n) := \max_{x: \left|x\right| = n} time_A(x)\]
	\textbf{Definition} (average-case)\\
	Bei der average-case Analyse wird angenommen, dass die Eingaben der Länge $n$ gleichverteilt sind. Damit ist die Laufzeit eine Zufallsvariable $T_{A,n}$. \[av-time_{A}(n) := \mathbb{E}[T_{A,n}]\] Mit der Definition des Erwartungswertes der Gleichverteilung also \[av-time_A(n) = \frac{1}{\left|\{x: \left|x\right| = n\}\right|} \cdot \sum_{x: \left|x\right| = n} time_A(n)\]
	\subsection{Abschätzungen}
	Die harmonische Reihe der ersten $n$ Glieder lässt sich abschätzen durch \[H_n = \sum_{i=1}^{n} \frac{1}{i} \leq \ln(n)+1\]
	\textbf{Definition} (Stirling Formel)
	\[n! \sim \sqrt{2\pi n} (\frac{n}{e})^n\]
	wobei $f(n) \sim g(n) \Leftrightarrow \lim_{n\to\infty} \frac{f(n)}{g(n)} = 1$. Daraus folgt auch \[\log(n!) \sim n\log(n) - n\log(e) + \log(\sqrt{2\pi n})\]
	\subsection{Rekursionsgleichungen}
	Zum Beispiel kann die Fibonacci Folge durch $F(n) = \Omega(2^{n/2})$ von unten abgeschätzt werden, da $F(n) \geq 2F(n-1)$.\\
	\textbf{Behauptung}: $F(n) \geq ac^n$ für $a,c >0$.\\ Beweis durch Induktion liefert $c \leq \phi = \frac{\sqrt{5}+1}{2}$. Insgesamt folgt $F(n) = \Theta((\frac{\sqrt{5}+1}{2})^n)$.\\
	\textbf{Beispiel} (Divide and Conquer)\\
	Das zu lösende Problem wird in zwei Teile mit geringerer Größe geteilt. Die Lösung für ein Problem setzt sich aus der Lösung für alle Teilprobleme zusammen. Die Laufzeit ist durch $\mathcal{O}(n\log(n))$ beschränkt. Beispiele für Divide and Conquer Verfahren sind Quicksort, Mergesort oder Binäre Suche.\\
	Mergesort: eine Eingabefolge $a_n$ wird in zwei gleich große Teile gespalten und die beiden Hälften rekursiv sortiert. Im Basisfall werden die beiden Hälften kombiniert und sortiert (merging).\newpage
	\textbf{Algorithmus}
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\State\texttt{INPUT: $x$
			\If{$\left|x\right|\leq 1$}
			\State Berechne die Lösung $L$ für $x$ direkt
			\Else $x = x_1 \cup x_2$
			\State $L_1$ = DivideAndConquer($x_1$)
			\State $L_2$ = DivideAndConquer($x_2$)
			\State Berechne $L$ aus $L_1$ und $L_2$
			\EndIf
			\State RETURN $L$}
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	Für den Aufwand $f$ folgt $f(n) = 2\cdot f(\frac{n}{2}) + c\cdot n + d$.	\\
	\textbf{Behauptung}: $f(n) = \alpha + \beta n +\delta n \log(n)$\\
	Der Beweis folgt durch elementares Nachrechnen. Es folgt durch Koeffizientenvergleich $\alpha = -d$ und $\delta = c$. Für den Mergesort erhält man zum Beispiel $f(n) = n\log(n) - n +1$.\\
	Die Berechnung kann auch durch Iterationsmethode berechnet werden. Dabei wird die Rekursionsgleichung immer wieder in sich selbst eingesetzt. Diese Einsetzung kann $log(n)$ oft geschehen, dann ist der Basisfall 1 erreicht. Mittels der $k$-ten Partialsumme der geometrischen Reihe, kann die Laufzeit bestimmt werden.\\
	\textbf{Beispiel} (Andere Rekursionen)\\
	Zum Beispiel für Probleme der Form \[f(n) = 2f(\sqrt{n}) + \log(n)\] hilft Iteration nicht weiter. Aber man kann sich durch Variablensubstitution behelfen. Mit $n = 2^m$ erhält man \[f(2^m) = 2f(2^{m/2}) + m\] Mit $f(2^m) = g(m)$ also \[g(m) = 2g(\frac{m}{2}) + m\]
	\textbf{Vereinfachung}:\begin{itemize}
		\item ganzzahlige Rekursionen $f(n) = f(\lfloor n\rfloor) + f(\lceil n \rceil)$ können ignoriert werden und von der Form $f(n) = 2f(\frac{n}{2})$ geschrieben werden.
		\item Ignorieren der Anfangswertbedingung
		\item Nur $\Omega$ und $\mathcal{O}$ Notation bestimmen
		\item Abschätzungen, die nur für große $n$ gelten
		\item \underline{Vorsicht:} asymptotische Notation in der Rekursiongleichung kann zu Problemen führen. 
	\end{itemize}
	\subsection{Master Theorem}
	\textbf{Satz}\\
	Gegeben eine Rekursiongleichung der Form \[T(n) = \sum_{i=1}^m T(\alpha_i\cdot n) + \Theta(n^k)\] wobei $0 \leq \alpha_i \leq 1$, $m\geq 1$ und $k\geq 0$. Dann kann man $T(n)$ abschätzen durch \[T(n) = \begin{cases}
		\Theta(n^k), & \text{falls } \sum_{i=1}^m \alpha_i^k < 1\\
		\Theta(n^k \log n), & \text{falls } \sum_{i=1}^m \alpha_i^k = 1\\
		\Theta(n^c), & \text{sonst}
	\end{cases}\]
	$c$ im letzten Fall ergibt sich dabei durch Lösen der folgenden Gleichung: \[\sum_{i=1}^m \alpha_i^c = 1\] Wenn alle $\alpha_i$ gleich sind, ergibt sich der Spezialfall $c = \frac{-\ln m}{\ln \alpha}$\\
	\textbf{Selektionsalgorithmen}\\
	Ein Selektionsalgorithmus soll für eine Folge $a_n$ und ein $i$ feststellen, welches das $i$-kleinste Element von $a_n$ ist. Zum Beispiel für $i=n$ das Maximum, $i=1$ das Minimum und $i=\frac{n}{2}$ der Median. Der naive Ansatz, die Liste zuerst zu sortieren, kann aber laufzeittechnisch verbessert werden. Als Komplexitätsmaß wird die Vergleichskomplexität $V(n)$ herangezogen. Im worst-case muss jeder Sortieralgorithmus so viele Vergleiche machen, wie man benötigt, um $n!$ durch halbieren zu einer Zahl $\leq 1$ zu machen. Mit der Stirling Formel ergibt sich, dass jeder vergleichsbasierte Sortieralgorithmus eine worst-case Laufzeit von $\geq n\log n - \Theta(n)$ hat. Die Darstellung eines solches Verfahren kann mit einem Entscheidungsbaum geschehen.\\
	\underline{\textbf{Heap Sort}}\\
	Ein Heap ist ein Binärbaum mit folgenden beiden Eigenschaftem \begin{itemize}
		\item der Baum ist soweit es geht vollständig gefüllt, die unterste Stufe ist von links gefüllt.
		\item Die Knoten haben Zahlen. Die Kindknoten eines beliebigen Knotens haben Werte kleiner oder gleich des Wertes des Elternknotens.
	\end{itemize}
	Der HeapSort läuft in zwei Phasen: \begin{enumerate}
		\item die Zahlenfolge wird zu einem Heap gebaut
		\item der Heap wird abgebaut, indem die Wurzel entfernt wird und ein anderer Knoten an ihre Stelle gesetzt wird, sodass die Heap Eigenschaft weiter erfüllt ist.
	\end{enumerate}
	HeapSort ist ein Inplace-Sortierverfahren. Die worst-case-Laufzeit beträgt $\mathcal{O}(n\log n)$.\\
	\underline{\textbf{Quicksort}}\\
	Wir betrachten Quicksort, ein stabiles divide-and-conquer Verfahren.
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\State\texttt{PROCEDURE} Quicksort($a_1,...,a_n$)
		\If{$n=0$}
			\State RETURN
		\EndIf
		\State $x\gets a_1$
		\State $l \gets$ leere Folge
		\State $r \gets$ leere Folge
		\For{$i=2$ TO $n$}
			{\If{$a_i \leq x$}
				\State{append $a_i$ to $l$} 
			\Else
				\State {append $a_i$ to r}
			\EndIf}
		\EndFor
		\State RETURN (QuickSort(l),$x$,QuickSort(r))
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\underline{Eigenschaften} \begin{itemize}
		\item im schlechtesten Fall wird jedes Mal die größte (oder kleinste) Zahl als Pivotelement gewählt
		\item Dann hat die $i$-te Teilfolge Läge $n-i$
		\item im schlimmsten Fall also eine Laufzeit von $\Theta(n^2)$
		\item im best-case ist das Pivotelement der Median
		\item die $i$-te Teilfolge hat dann Länge $\frac{n}{2^i}$ 
		\item das bringt eine Laufzeit von $n\log n + \Theta(n)$
	\end{itemize}
	\subsection{Average-Case-Analyse von Quicksort}
	Annahme: die Eingabe Liste ist gleichverteilt und paarweise verschieden.\\
	Die Eingabefolge sein $a_n$ und die sortierte Folge $s_n$. Wir führen eine Zufallsvariable $X_{ij}$ ein, wobei $X_{ij} = 1$, falls beim Quicksort $s_i$ mit $s_j$ verglichen werden. Daher ist die mittlere Anzahl an Vergleichen gegeben durch \[V(n) = \sum_{i<j}\mathbb{E}[X_{ij}]\]
	Nun ist $\mathbb{E}[X_{ij}] = p_{ij} + 0\cdot (1-p_{ij}) = p_{ij}$ die Wahrscheinlichkeit, dass $s_i$ und $s_j$ verglichen werden. Man findet, dass $p_{ij} = \frac{2}{j-i+1}$. Insgesamt also \[V(n) = \sum_{i<j} \mathbb{E}[X_{ij}] = \sum_{i<j} \frac{2}{j-i+1}\] Durch elementares Umformen findet man $V(n) \leq 1,3863n \log_2 n$. Die obige Implementierung von Quicksort ist nicht in-place, denn es werden immer neue (Teil-)Folgen generiert.
	\section{Selektionsalgorithmen}
	Gegeben sei eine Folge von Zahlen. Es soll das $i$-kleinste Element gefunden werden für $i \in [n]$. Bei $i=1$ das Minimum, bei $i=n$ das Maximum, bei $i = \frac{n}{2}$ den Median.\\
	\subsection{Spezialfall Extrema (z.B. Maximum)} 
	Die naive Implementierung braucht lineare Zeit und genauer $V(n) = n-1$ Vergleiche. Diese Implementierung ist optimal, denn bei weniger als $n-1$ Vergleichen gibt es ein Element, das nie überprüft wurde.
	\subsection{Maximum und Minimum}
	Die naive Implementierung bestimmt die Extrema separat und benötigt daher $2n-2$ Vergleiche. Dieser Wert lässt sich verbessern:\\ Es wird immer ein Element mit seinem Nachfolger verglichen. Dabei gibt es $\frac{n}{2}$ größere und $\frac{n}{2}$ kleinere Elemente. Für diese Partition braucht man daher $\frac{n}{2}$ Vergleiche. Unter den kleineren kann das Minimum und unter den größeren das Maximum in jeweils $\frac{n}{2}-1$ Vergleichen bestimmt werden. Insgesamt folgt also $V(n) = \frac{3}{2}n-2$. Man kann zeigen, dass dieses Vorgehen optimal ist.
	\subsection{Selektion des $k$-kleinsten Elementes}
	Die Vorgehenweise ist angelegt an Quicksort. Folgende Prozedur löst das Problem:\\
	Select($a,l,r,i$) bestimmt das $i$-kleinste Element in der Teilfolge $a[l,r]$.\\
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}
		\If{$i+l-1\notin \{l,...,r\}$} 
		\State return fail
		\EndIf
		\State Bestimmte Pivotelement $x$ aus $a[l,...,r]$
		\State Sei $m$ der Rang von $x$ in $a[l,...,r]$
		\State sortiere $a$ so um, dass $a[l+m-1]=x \land \begin{cases}
			a[j] < a[m+l-1] & \text{ für } l\leq j<m+l-1\\
			a[j] > a[m+l-1] & \text{ für } r \geq j >m+l-1 
		\end{cases}$
	\If{$i=m+l-1$}
	\State return $x$
	\EndIf
	\If{$i<m+l-1$}
	\State return Select($a,l,l+m-2,i$)
	\EndIf
	\If{$i>m+l-1$}
	\State return Select($a,l+m,r,i-m$)
	\EndIf
	\end{algorithmic}
	\par\noindent\rule{\textwidth}{0.4pt}
	\textbf{Worst-Case-Komplexität}\\
	Analog zum Quicksort tritt der schlechteste Fall ein, wenn das Pivotelement am Rand liegt. Dann ist die Komplexität $\mathcal{O}(n^2)$.\\
	\textbf{Random Select}\\
	Angenommen das Pivotelement wird zufällig gemäß einer Gleichverteilung gewählt. Ist $x$ das $j$-kleinste Element so hat das linke Teilarray die Größe $j-1$ und das rechte $n-j$. Im schlimmsten Fall wird Select rekursiv mit dem größeren Teilarray aufgerufen. die Aufteilung benötigt $n-1$ Vergleiche. D.h. die mittlere Anzahl von Vergleichen ist rekursiv beschränkt durch $V(n) \leq (n-1) + \frac{2}{n} \sum_{j = n/2}^{n-1} V(j)$. Durch Einsetzen kann man diese Rekursionsungleichung lösen und findet, dass der average case linear ist.\\
	\textbf{Pivotelement geschickt wählen}\\
	Man organisiert die Liste in Fünfergruppen und sortiert diese (jeweils konstant mit genau 7 Vergleichen). In jeder Fünfergruppe wird der Median bestimmt und anschließend der Median dieser Mediane. Dieser wird als Pivotelement gesetzt. Damit hat sich die Anzahl der zu vergleichenden Elemente auf $\frac{4}{10}n$. Es ergibt sich die Rekursionsgleichung \[V(n) = \frac{7n}{5} + V(\frac{n}{5} + \frac{4n}{10}) + V(\frac{7n}{10})\] Mittels Mastertheorem lässt sich diese Gleichung lösen und man findet, dass die Prozedur mit dieser Pivotwahl linearen Aufwand hat.
	\section{Hashing}
	Hashing ist ein System zur dynamischen Verwaltung von Daten, die über einen eindeutigen Schlüssel angesprochen werden. Das Speicherverfahren bietet einige Vorteile:\begin{itemize}
		\item Die Daten können in Bäumen verwaltet werden, wodurch die Dictionary Operations Aufwand von $\mathcal{O}(\log n)$ erhalten. Beim Hashing beschränkt sich dieser Aufwand auf $\mathcal{O}(1)$.
		\item Hilfreich, wenn es viele potentielle Schlüssel $U$ gibt und
		\item die Menge $S$ der Schlüssel aber klein im Vergleich zu $U$ ist.
		\item Außerdem, wenn $S$ im Vorhinein nicht bekannt ist.
	\end{itemize}	
	\textbf{Definition}\\
	Eine Hashtabelle ist ein Array von Zeigern auf die eigentlichen Datensätze. \\
	\textbf{Definition}\\
	Eine Hashfunktion $h$ bildet die Schlüssel auf natürliche Zahlen ab.\\
	
	Wenn die Hashfunktion injektiv ist, können die Dictionary Operations Suchen, Einfügen, Entfernen direkt implementiert werden. Gilt für $s,s' \in S$, dass $h(s) = h(s')$, so spricht man von einer Kollision.
	\subsection{Hashfunktionen}
	\subsubsection{Divisions-/Kongruenzmethode}
	$h(s) = s \mod m$, im besten Fall sollte $m$ prim sein.
	\subsubsection{Multiplikationsmethode}
	$h(s) = \lfloor m \cdot ((s\cdot \alpha) \mod 1)\rfloor$ für eine reelle Zahl $\alpha \in [0,1]$. $\mod 1$ soll den ganzzahligen Anteil streichen und $m = 2^p$ für eine natürliche Zahl $p$.\\
	
	Kollisionen lassen sich meist nicht verhindern. Das kann als Analogie zum Geburtstagsparadoxon gesehen werden. Für eine Hashtabelle der Größe $m$ und $n$ Schlüssel können wir die Anzahl der Kollisionen mit folgender Zufallsvariablen abschätzen: \[X_{i,j} = \begin{cases}
		1, & h(s_i) = h(s_j)\\
		0, & \text{ sonst}
	\end{cases}\]
	Für $i\neq j$ gilt dann $\mathbb{E}[X_{i,j}] = \frac{1}{m}$. Sei $X = \sum_{i<j} X_{i,j}$, dann gilt $\mathbb{E}[X] = \binom{n}{2} \frac{1}{m}$. 
	\subsection{Belegungsfaktor}
	Der Quotient $\beta = \frac{n}{m}$ heißt der Belegungsfaktor einer Hashtabelle der Größe $m$, wobei $n$ die Anzahl der gespeicherten Elemente ist. Der Suchaufwand lässt sich als Funktion in $\beta$ abschätzen.
	\subsection{Hashing mit Verkettung}
	Jedes Element der der Hashtabelle ist Ausgangspunkt einer Überlaufliste. In dieser Liste werden wiederum linked lists gespeichert, deren Schlüssel den selben Hashwert erzeugen. In diesem Fall sind die Dictionary Operations Initialisierung, Suchen, Einfügen und Löschen leicht realisierbar. Für die Komplexitätsanalyse der Suche unterscheiden wir zwei Komplexitätsfunktionen: \begin{enumerate}
		\item $A(n,m)$, die mittlere Anzahl der Sondierschritte, vorausgesetzt der Schlüssel ist in der Tabelle eingetragen.
		\item $A'(n,m)$, vorausgesetzt der Schlüssel ist nicht in der Tabelle eingetragen. 
	\end{enumerate}
	Man kann zeigen, dass der Aufwand bei erfolgloser Suche $A' = 1+\beta$ beträgt und bei erfolgreicher Suche durch $A \leq 1+\frac{\beta}{2}$ abgeschätzt werden kann.
	\subsection{Open Hashing}
	Hierbei können maximal $n=m$ Elemente verwaltet werden, d.h. $\beta \leq 1$. Das Löschen fällt in dieser Form schwerer, da die Sondierreihenfolge durch eine gelöschte Stelle unterbrochen werden kann. Eine Möglichkeit besteht darin, zwischen freien und gelöschten Stellen zu unterscheiden. Der Suchaufwand beim linearen Sondieren beträgt $A \approx \frac{1}{2} \left(1+ \frac{1}{1-\beta} \right)$ bzw. $A' = \frac{1}{2} \left(1+\frac{1}{(1-\beta)^2}\right)$. 
	\subsection{Double Hashing}
	Zur Vermeidung von Clusterbildung legt eine zweite Hashfunktion $h'$ die Schrittweite für die Sondierung fest. Zunächst wird $h(s)$ sondiert und dann iterativ die Adressen $(h(s) + i\cdot h'(s)) \mod m$ für $i \in \mathbb{N}$ nach einem freien Platz abgesucht. Lineares Sondieren ist hier also der Spezialfall $h' \equiv 1$. $h$ und $h'$ sollten sich möglichst stark voneinander unterscheiden. Eigenschaften beim Double Hashing \begin{itemize}
		\item Es ist wichtig, dass $m$ eine Primzahl ist
		\item Wenn $m$ und $h'(s)$ nicht teilerfremd sind, so wird beim Sondieren nicht die gesamte Tabelle durchlaufen
		\item $A' = \frac{1}{1-\beta}$
		\item $A = -\frac{1}{\beta} \cdot \ln(1-\beta)$
	\end{itemize}
	\subsection{Bloomfilter}
	Das ist eine Datenstruktur, mit der effizient festgestellt werden kann, ob ein Element bereits gespeichert wurde oder nicht. Er besteht aus einem Bitarray der Größe $m$ das mit Nullen initialisiert wird und $k$ unterschiedlichen Hashfunktionen mit $h_i: U \to \{0,...,m-1\}$.\\
	Soll nun $x$ gespeichert werden, so werden zunächst die $k$ Hashwerte $h_i(x)$ bestimmt. Jede dieser Positionen im Bitarray $A$ wird auf 1 gesetzt.\\
	Nun soll überprüft werden, ob ein Wert $w$ gespeichert ist. \begin{enumerate}
		\item Zuerst werden wieder die $k$ Hashwerte bestimmt.
		\item Enthält eine der Positionen eine 0, so ist $w$ nicht gespeichert worden.
		\item Enthalten alle eine 1, so ist es wahrscheinlich, dass $w$ gespeichert wurde, aber nicht sicher (falsch positive möglich). 
	\end{enumerate} 
	\section{Funktionale Programmierung}
	\textbf{Beispiel}\\
	Der Binomialkoeffizient lässt sich rekursiv wie folgt berechnen: \[\binom{n}{k} = \binom{n-1}{k-1} + \binom{n-1}{k}\]
	Setzt man $b(n,k) = \binom{n}{k}$ erhält man folgende Rekursionsvorschrift \[b(n,k) = \begin{cases}
		1,& \text{falls } k=0 \lor n=k\\
		b(n-1, k-1) + b(n-1, k),& \text{sonst}
	\end{cases}\] 
	Mittels dieses Verfahrens erhält man einen Aufwand von $\Theta(\frac{2^n}{\sqrt{n}})$. Der Aufwand kann reduziert werden, wenn die schon berechneten Werte in einer Tabelle $t[n,k]$ gespeichert werden. Wir können die Anzahl der Additionen auf $n*k$ reduzieren.\\
	
	Im Gegensatz zu den rekursiven top-down Ansätzen wird beim dynamischen Programmieren die Lösung in bottom-up berechnet. Begonnen wird bei den Randwerten, die späteren Lösungen werden rekursiv aus den kleineren Teillösungen berechnet. Im Beispiel des Binomialkoeffizienten erhält man das Pascal'sche Dreieck. Allgemein ist der Aufwand bei dynamischer Programmierung beschränkt durch (Größe der Tabelle)(Aufwand zur Berechnung eines Eintrages). Eingesetzt wird dieses Prinzip meist zur Lösung von Optimierungsproblemen. Dafür ist es wichtig, dass ein Problem dem Bellmanschen Optimalitätsprinzip folgt: \begin{quote}
		Die optimale Lösung für Probleme setzt der Größe n setzt sich aus optimalen Lösungen für kleinere Teilprobleme zusammen.
	\end{quote}
	Die Entwicklung eines Algorithmus erfolgt in mehreren Schritten \begin{enumerate}
		\item Charakterisiere den Lösungsraum und die Struktur einer optimalen Lösung
		\item Definiere rekursiv, wie sich eine optimale Lösung aus optimalen Teillösungen ermitteln lässt.
		\item Konzipiere eine Algorithmus, der einer Tabelle verwendet.
	\end{enumerate}
	\textbf{Definition}\\
	Die Editierdistanz $d(S_1, S_2)$ ist die minimale Anzahl an Buchstabenoperationen, die nötig sind, um den String $S_1$ in $S_2$ umzuwandeln. Man kann Buchstaben einsetzen, löschen und ersetzen. Mit der naiven Strategie stellt man fest, dass der Aufwand exponentiell wächst, während man mittels dynamischer Programmierung eine Komplexität von $\mathcal{O}(n^3)$ erreicht.\\
	\subsection{Traveling Salesman Problem}
	Das Problem ist \textbf{NP}-vollständig. Der naive Ansatz hat eine Komplexität von $\Omega(n!)$. Wir betrachten im Folgenden das Problem in einem gerichteten Graphen, d.h. die Entfernungsmatrix ist i.A. nicht symmetrisch und die Dreiecksungleichung muss nicht erfüllt sein. Wir betrachten das Problem rekursiv: Wir beginnen bei Stadt $1$, finden eine Rundreise in $\{2,...,k-1\}$ reisen zu $k$ und dann zurück zu $1$. Durch dieses Verfahren wird der Aufwand beschränkt durch $\mathcal{O}(n^22^n)$.
	\subsection{Knapsack Problem}
	Auch dieses Problem ist \textbf{NP}-vollständig. Sei $I$ die Menge der Indizes, sodass die Lösung, die $I$ entspricht optimal sei. Für $i \in I$ ist das Problem mit Größe $G-g_i$ und Indexmenge $I\setminus\{i\}$ optimal gefüllt. Mittels dynamischer Programmierung kann man eine Komplexität von $\mathcal{O}(n2^k)$ erreichen.  
\end{document}